{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_dataset_size(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE):\n",
    "    if dataset_size == DATASET_SIZE_TYPE.full:\n",
    "        if dataset_type == DATASET_TYPE.train:\n",
    "            return FULL_TRAIN_DATASET_SIZE\n",
    "        else:\n",
    "            return FULL_TEST_DATASET_SIZE\n",
    "    else:\n",
    "        if dataset_type == DATASET_TYPE.train:\n",
    "            return SMALL_TRAIN_DATASET_SIZE\n",
    "        else:\n",
    "            return SMALL_TEST_DATASET_SIZE\n",
    "        \n",
    "def get_steps_per_epoch(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE):\n",
    "    return get_dataset_size(dataset_size, dataset_type) // BATCH_SIZE\n",
    "\n",
    "def get_max_steps():\n",
    "    global EPOCHS\n",
    "    return EPOCHS * get_training_steps_per_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from google.cloud import bigquery\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_mean_and_std_dicts():\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    query = \"\"\"\n",
    "    select\n",
    "    AVG(int1) as avg_int1, STDDEV(int1) as std_int1,\n",
    "    AVG(int2) as avg_int2, STDDEV(int2) as std_int2,\n",
    "    AVG(int3) as avg_int3, STDDEV(int3) as std_int3,\n",
    "    AVG(int4) as avg_int4, STDDEV(int4) as std_int4,\n",
    "    AVG(int5) as avg_int5, STDDEV(int5) as std_int5,\n",
    "    AVG(int6) as avg_int6, STDDEV(int6) as std_int6,\n",
    "    AVG(int7) as avg_int7, STDDEV(int7) as std_int7,\n",
    "    AVG(int8) as avg_int8, STDDEV(int8) as std_int8,\n",
    "    AVG(int9) as avg_int9, STDDEV(int9) as std_int9,\n",
    "    AVG(int10) as avg_int10, STDDEV(int10) as std_int10,\n",
    "    AVG(int11) as avg_int11, STDDEV(int11) as std_int11,\n",
    "    AVG(int12) as avg_int12, STDDEV(int12) as std_int12,\n",
    "    AVG(int13) as avg_int13, STDDEV(int13) as std_int13\n",
    "    from `{project}.{dataset}.{table_name}`\n",
    "  \"\"\".format(\n",
    "        project=PROJECT_ID,\n",
    "        dataset=DATASET_ID,\n",
    "        table_name='days',\n",
    "    )\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=LOCATION,\n",
    "    )  # API request - starts the query\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "\n",
    "    mean_dict = dict((field[0].replace('avg_', ''), df[field[0]][0])\n",
    "                     for field in df.items() if field[0].startswith('avg'))\n",
    "    std_dict = dict((field[0].replace('std_', ''), df[field[0]][0])\n",
    "                    for field in df.items() if field[0].startswith('std'))\n",
    "    return (mean_dict, std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "get_mean_and_std_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from google.cloud import bigquery\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_vocabulary_size_dict():\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "    COUNT(DISTINCT cat1) as cat1,\n",
    "    COUNT(DISTINCT cat2) as cat2,\n",
    "    COUNT(DISTINCT cat3) as cat3,\n",
    "    COUNT(DISTINCT cat4) as cat4,\n",
    "    COUNT(DISTINCT cat5) as cat5,\n",
    "    COUNT(DISTINCT cat6) as cat6,\n",
    "    COUNT(DISTINCT cat7) as cat7,\n",
    "    COUNT(DISTINCT cat8) as cat8,\n",
    "    COUNT(DISTINCT cat9) as cat9,\n",
    "    COUNT(DISTINCT cat10) as cat10,\n",
    "    COUNT(DISTINCT cat11) as cat11,\n",
    "    COUNT(DISTINCT cat12) as cat12,\n",
    "    COUNT(DISTINCT cat13) as cat13,\n",
    "    COUNT(DISTINCT cat14) as cat14,\n",
    "    COUNT(DISTINCT cat15) as cat15,\n",
    "    COUNT(DISTINCT cat16) as cat16,\n",
    "    COUNT(DISTINCT cat17) as cat17,\n",
    "    COUNT(DISTINCT cat18) as cat18,\n",
    "    COUNT(DISTINCT cat19) as cat19,\n",
    "    COUNT(DISTINCT cat20) as cat20,\n",
    "    COUNT(DISTINCT cat21) as cat21,\n",
    "    COUNT(DISTINCT cat22) as cat22,\n",
    "    COUNT(DISTINCT cat23) as cat23,\n",
    "    COUNT(DISTINCT cat24) as cat24,\n",
    "    COUNT(DISTINCT cat25) as cat25,\n",
    "    COUNT(DISTINCT cat26) as cat26\n",
    "    FROM\n",
    "      from `{project}.{dataset}.{table_name}`\n",
    "  \"\"\".format(\n",
    "        project=PROJECT_ID,\n",
    "        dataset=DATASET_ID,\n",
    "        table_name='days',\n",
    "    )\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=LOCATION,\n",
    "    )  # API request - starts the query\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "    dictionary = dict((field[0], df[field[0]][0]) for field in df.items())\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "get_vocabulary_size_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from google.cloud import bigquery\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "#TODO: add step to generate corpus and update script to use right table\n",
    "\n",
    "def get_corpus_dict():\n",
    "    table_name = 'days' if DATASET_SIZE == DATASET_SIZE_TYPE.full else 'small'\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    query = \"\"\"\n",
    "    select\n",
    "    cat_name,\n",
    "    cat_value,\n",
    "    cat_index\n",
    "    from `alekseyv-scalableai-dev.criteo_kaggle.{table_name}_corpus`\n",
    "  \"\"\".format(table_name=table_name)\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=\"US\",\n",
    "    )  # API request - starts the query\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "    corpus = dict()\n",
    "    for _, row in df.iterrows():\n",
    "        cat_name = row[0]\n",
    "        cat_value = row[1]\n",
    "        cat_index = row[2]\n",
    "        if not cat_name in corpus:\n",
    "            corpus[cat_name] = dict()\n",
    "        if cat_value is None:\n",
    "            cat_value = ''\n",
    "        corpus[cat_name][cat_value] = cat_index\n",
    "    return corpus\n",
    "\n",
    "def corpus_to_lookuptable(corpus):\n",
    "    lookup_dict = dict()\n",
    "    for key, value in corpus.items():\n",
    "        initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "            list(value.keys()),\n",
    "            list(value.values()),\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64)\n",
    "        # cat_index in corpus starts with 1, reserving 0 for out of vocabulary values\n",
    "        lookup_table = tf.lookup.StaticHashTable(initializer, 0)\n",
    "        lookup_dict[key] = lookup_table\n",
    "    return lookup_dict\n",
    "\n",
    "\n",
    "def get_corpus(embeddings_mode):\n",
    "    if embeddings_mode == EMBEDDINGS_MODE_TYPE.manual or embeddings_mode == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "        return corpus_to_lookuptable(get_corpus_dict())\n",
    "    else:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat1': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f98503684e0>,\n",
       " 'cat2': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffb00>,\n",
       " 'cat3': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff940>,\n",
       " 'cat4': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff780>,\n",
       " 'cat5': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff5c0>,\n",
       " 'cat6': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff400>,\n",
       " 'cat7': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff240>,\n",
       " 'cat8': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff048>,\n",
       " 'cat9': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff208>,\n",
       " 'cat10': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff3c8>,\n",
       " 'cat11': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff588>,\n",
       " 'cat12': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff748>,\n",
       " 'cat13': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff908>,\n",
       " 'cat14': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffac8>,\n",
       " 'cat15': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffc88>,\n",
       " 'cat16': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffe48>,\n",
       " 'cat17': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6ffd0>,\n",
       " 'cat18': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6fe10>,\n",
       " 'cat19': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6fc50>,\n",
       " 'cat20': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6fb00>,\n",
       " 'cat21': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f8d0>,\n",
       " 'cat22': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f710>,\n",
       " 'cat23': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f588>,\n",
       " 'cat24': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f3c8>,\n",
       " 'cat25': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f208>,\n",
       " 'cat26': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f080>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "get_corpus(EMBEDDINGS_MODE_TYPE.manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.data.experimental.ops import interleave_ops\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)\n",
    "\n",
    "# @tf.function is not really necessary here since function is used in tf.data pipeline\n",
    "# Use print(tf.autograph.to_code(transform_row.python_function)) to see code generated by AutoGraph\n",
    "@tf.function\n",
    "def transform_row(row_dict, mean_dict, std_dict, corpus, embeddings_mode:EMBEDDINGS_MODE_TYPE):\n",
    "    dict_without_label = dict(row_dict)\n",
    "    label = dict_without_label.pop('label')\n",
    "    for field in CSV_SCHEMA:\n",
    "        if (field.name.startswith('int')):\n",
    "            # use normalized mean value if data is missing (value will be 0)\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = (\n",
    "                value - mean_dict[field.name]) / std_dict[field.name]\n",
    "        elif field.name.startswith('cat'):\n",
    "            if embeddings_mode == EMBEDDINGS_MODE_TYPE.none:\n",
    "                dict_without_label.pop(field.name)\n",
    "            elif embeddings_mode == EMBEDDINGS_MODE_TYPE.manual:\n",
    "                cat = dict_without_label[field.name]\n",
    "                if cat is None:\n",
    "                    cat = ''\n",
    "                cat_index = corpus[field.name].lookup(cat)\n",
    "                if cat_index is None:\n",
    "                    tf.print('not found for {}'.format(field.name))\n",
    "                    cat_index = tf.constant(-1)\n",
    "                dict_without_label[field.name] = cat_index\n",
    "    return (dict_without_label, label)\n",
    "\n",
    "def get_bigquery_table_name(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE):\n",
    "    return dataset_type.name + '_' + dataset_size.name\n",
    "\n",
    "def read_bigquery(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE, embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    table_name = get_bigquery_table_name(dataset_size, dataset_type)\n",
    "    (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "    corpus = get_corpus(embedding_mode)\n",
    "    requested_streams_count = 10\n",
    "    tensorflow_io_bigquery_client = BigQueryClient()\n",
    "    read_session = tensorflow_io_bigquery_client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, table_name, DATASET_ID,\n",
    "        list(field.name for field in CSV_SCHEMA),\n",
    "        list(dtypes.int64 if field.field_type == 'INTEGER'\n",
    "             else dtypes.string for field in CSV_SCHEMA),\n",
    "        requested_streams=requested_streams_count)\n",
    "\n",
    "    # manually sharding output instead of using return read_session.parallel_read_rows()\n",
    "    streams = read_session.get_streams()\n",
    "    # streams_count = len(streams) # does not work for Estimator\n",
    "    streams_count = tf.size(streams)\n",
    "    streams_count64 = tf.cast(streams_count, dtype=tf.int64)\n",
    "    streams_ds = dataset_ops.Dataset.from_tensor_slices(\n",
    "        streams).shuffle(buffer_size=streams_count64)\n",
    "    dataset = streams_ds.interleave(\n",
    "        read_session.read_rows,\n",
    "        cycle_length=streams_count64,\n",
    "        num_parallel_calls=streams_count64)\n",
    "\n",
    "    def transform_row_function(row): return transform_row(\n",
    "        row, mean_dict, std_dict, corpus, embedding_mode)\n",
    "\n",
    "    transformed_ds = dataset\\\n",
    "        .batch(BATCH_SIZE) \\\n",
    "        .shuffle(50) \\\n",
    "        .map(transform_row_function, num_parallel_calls=streams_count) \\\n",
    "        .prefetch(50)\n",
    "\n",
    "    # TODO: enable once tf.data.experimental.AutoShardPolicy.OFF is available\n",
    "    # Interleave dataset is not shardable, turning off sharding\n",
    "    # See https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size\n",
    "    # Instead we are shuffling data.\n",
    "    # options = tf.data.Options()\n",
    "    #  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    # return transformed_ds.with_options(options)\n",
    "    return transformed_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "#BATCH_SIZE = 4\n",
    "dataset = read_bigquery(DATASET_SIZE_TYPE.small, DATASET_TYPE.validation, EMBEDDINGS_MODE_TYPE.hashbucket)\n",
    "for row in dataset.take(2):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transofrom_row_gcs(row_tuple, mean_dict, std_dict, corpus):\n",
    "    row_dict = dict(\n",
    "        zip(list(field.name for field in CSV_SCHEMA) + ['row_hash'], list(row_tuple)))\n",
    "    row_dict.pop('row_hash')\n",
    "    return transform_row(row_dict, mean_dict, std_dict, corpus)\n",
    "\n",
    "\n",
    "def _get_file_names(file_pattern):\n",
    "    if isinstance(file_pattern, list):\n",
    "        if not file_pattern:\n",
    "            raise ValueError(\"File pattern is empty.\")\n",
    "        file_names = []\n",
    "        for entry in file_pattern:\n",
    "            file_names.extend(gfile.Glob(entry))\n",
    "    else:\n",
    "        file_names = list(gfile.Glob(file_pattern))\n",
    "\n",
    "    if not file_names:\n",
    "        raise ValueError(\"No files match %s.\" % file_pattern)\n",
    "    return file_names\n",
    "\n",
    "\n",
    "def read_gcs(table_name):\n",
    "    if DATASET_SIZE == DATASET_SIZE_TYPE.small:\n",
    "        table_name += '_small'\n",
    "    else:\n",
    "        table_name += '_full'\n",
    "\n",
    "    gcs_filename_glob = 'gs://alekseyv-scalableai-dev-public-bucket/criteo_kaggle_from_bq/{}*'.format(\n",
    "        table_name)\n",
    "    file_names = _get_file_names(gcs_filename_glob)\n",
    "    num_parallel_calls = max(10, len(file_names))\n",
    "    file_names_ds = dataset_ops.Dataset.from_tensor_slices(\n",
    "        file_names).shuffle(buffer_size=20)\n",
    "    record_defaults = list(tf.int32 if field.name == 'label' else tf.constant(0, dtype=tf.int32) if field.name.startswith(\n",
    "        'int') else tf.constant('', dtype=tf.string) for field in CSV_SCHEMA) + [tf.string]\n",
    "    dataset = file_names_ds.interleave(\n",
    "        lambda file_name: tf.data.experimental.CsvDataset(\n",
    "            file_name, record_defaults, field_delim='\\t', header=False),\n",
    "        cycle_length=num_parallel_calls,\n",
    "        num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    corpus = get_corpus()\n",
    "    (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "    transofrom_row_gcs_function = lambda *row_tuple: transofrom_row_gcs(\n",
    "        row_tuple, mean_dict, std_dict, corpus)\n",
    "\n",
    "    transformed_ds = dataset\\\n",
    "        .batch(BATCH_SIZE) \\\n",
    "        .shuffle(50) \\\n",
    "        .map(transofrom_row_gcs_function, num_parallel_calls=num_parallel_calls) \\\n",
    "        .prefetch(50)\n",
    "    return transformed_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_dataset(dataset_source: DATASET_SOURCE_TYPE, \n",
    "                dataset_size: DATASET_SIZE_TYPE, \n",
    "                dataset_type: DATASET_TYPE):\n",
    "    if DATASET_SOURCE == DATASET_SOURCE_TYPE.gcs:\n",
    "        return read_gcs(dataset_size, dataset_type)\n",
    "    else:\n",
    "        return read_bigquery(dataset_size, dataset_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
