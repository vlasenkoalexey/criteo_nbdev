{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_dataset_size(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE):\n",
    "    if dataset_size == DATASET_SIZE_TYPE.full:\n",
    "        if dataset_type == DATASET_TYPE.training:\n",
    "            return FULL_TRAINING_DATASET_SIZE\n",
    "        else:\n",
    "            return FULL_VALIDATION_DATASET_SIZE\n",
    "    elif dataset_size == DATASET_SIZE_TYPE.small:\n",
    "        if dataset_type == DATASET_TYPE.training:\n",
    "            return SMALL_TRAINING_DATASET_SIZE\n",
    "        else:\n",
    "            return SMALL_VALIDATION_DATASET_SIZE\n",
    "    elif dataset_size == DATASET_SIZE_TYPE.tiny:\n",
    "        if dataset_type == DATASET_TYPE.training:\n",
    "            return TINY_TRAINING_DATASET_SIZE\n",
    "        else:\n",
    "            return TINY_VALIDATION_DATASET_SIZE\n",
    "    else:\n",
    "        print('debug:')\n",
    "        print(dataset_size)\n",
    "        print(type(dataset_size))\n",
    "        print(type(dataset_size).__module__)\n",
    "        print(DATASET_SIZE_TYPE.tiny)\n",
    "        print(type(DATASET_SIZE_TYPE.tiny))\n",
    "        print(type(DATASET_SIZE_TYPE.tiny).__module__)\n",
    "        print(DATASET_SIZE_TYPE.tiny == dataset_size)\n",
    "        raise ValueError('Invalid dataset_size: %s' % dataset_size)\n",
    "        \n",
    "def get_steps_per_epoch(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE):\n",
    "    return get_dataset_size(dataset_size, dataset_type) // BATCH_SIZE\n",
    "\n",
    "def get_max_steps():\n",
    "    global EPOCHS\n",
    "    return EPOCHS * get_training_steps_per_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "get_steps_per_epoch(DATASET_SIZE_TYPE.tiny, DATASET_TYPE.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "get_steps_per_epoch(DATASET_SIZE_TYPE.tiny, DATASET_TYPE.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from google.cloud import bigquery\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_mean_and_std_dicts():\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    query = \"\"\"\n",
    "    select\n",
    "    AVG(int1) as avg_int1, STDDEV(int1) as std_int1,\n",
    "    AVG(int2) as avg_int2, STDDEV(int2) as std_int2,\n",
    "    AVG(int3) as avg_int3, STDDEV(int3) as std_int3,\n",
    "    AVG(int4) as avg_int4, STDDEV(int4) as std_int4,\n",
    "    AVG(int5) as avg_int5, STDDEV(int5) as std_int5,\n",
    "    AVG(int6) as avg_int6, STDDEV(int6) as std_int6,\n",
    "    AVG(int7) as avg_int7, STDDEV(int7) as std_int7,\n",
    "    AVG(int8) as avg_int8, STDDEV(int8) as std_int8,\n",
    "    AVG(int9) as avg_int9, STDDEV(int9) as std_int9,\n",
    "    AVG(int10) as avg_int10, STDDEV(int10) as std_int10,\n",
    "    AVG(int11) as avg_int11, STDDEV(int11) as std_int11,\n",
    "    AVG(int12) as avg_int12, STDDEV(int12) as std_int12,\n",
    "    AVG(int13) as avg_int13, STDDEV(int13) as std_int13\n",
    "    FROM `{project}.{dataset}.{table_name}`\n",
    "  \"\"\".format(\n",
    "        project=PROJECT_ID,\n",
    "        dataset=DATASET_ID,\n",
    "        table_name='days',\n",
    "    )\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=LOCATION,\n",
    "    )  # API request - starts the query\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "\n",
    "    mean_dict = dict((field[0].replace('avg_', ''), float(df[field[0]][0]))\n",
    "                     for field in df.items() if field[0].startswith('avg'))\n",
    "    std_dict = dict((field[0].replace('std_', ''), float(df[field[0]][0]))\n",
    "                    for field in df.items() if field[0].startswith('std'))\n",
    "    return (mean_dict, std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/alekseyv/.local/lib/python3.8/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'int1': 3.5024133170753995,\n",
       "  'int2': 105.8484197976657,\n",
       "  'int3': 26.91304102061112,\n",
       "  'int4': 7.322680248873331,\n",
       "  'int5': 18538.99166487135,\n",
       "  'int6': 116.06185085211605,\n",
       "  'int7': 16.333130032135013,\n",
       "  'int8': 12.517042137556762,\n",
       "  'int9': 106.10982343805145,\n",
       "  'int10': 0.6175294977722183,\n",
       "  'int11': 2.7328343170173173,\n",
       "  'int12': 0.9910356287721245,\n",
       "  'int13': 8.21746116117401},\n",
       " {'int1': 9.429076407105086,\n",
       "  'int2': 391.4578226870704,\n",
       "  'int3': 397.97258302273474,\n",
       "  'int4': 8.793230712645805,\n",
       "  'int5': 69394.60184622335,\n",
       "  'int6': 382.5664493712363,\n",
       "  'int7': 66.0497552451171,\n",
       "  'int8': 16.688884567787586,\n",
       "  'int9': 220.28309398647906,\n",
       "  'int10': 0.6840505553977025,\n",
       "  'int11': 5.199070884811354,\n",
       "  'int12': 5.597723872237179,\n",
       "  'int13': 16.211932558173785})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "get_mean_and_std_dicts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from google.cloud import bigquery\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_vocabulary_size_dict():\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "    COUNT(DISTINCT cat1) as cat1,\n",
    "    COUNT(DISTINCT cat2) as cat2,\n",
    "    COUNT(DISTINCT cat3) as cat3,\n",
    "    COUNT(DISTINCT cat4) as cat4,\n",
    "    COUNT(DISTINCT cat5) as cat5,\n",
    "    COUNT(DISTINCT cat6) as cat6,\n",
    "    COUNT(DISTINCT cat7) as cat7,\n",
    "    COUNT(DISTINCT cat8) as cat8,\n",
    "    COUNT(DISTINCT cat9) as cat9,\n",
    "    COUNT(DISTINCT cat10) as cat10,\n",
    "    COUNT(DISTINCT cat11) as cat11,\n",
    "    COUNT(DISTINCT cat12) as cat12,\n",
    "    COUNT(DISTINCT cat13) as cat13,\n",
    "    COUNT(DISTINCT cat14) as cat14,\n",
    "    COUNT(DISTINCT cat15) as cat15,\n",
    "    COUNT(DISTINCT cat16) as cat16,\n",
    "    COUNT(DISTINCT cat17) as cat17,\n",
    "    COUNT(DISTINCT cat18) as cat18,\n",
    "    COUNT(DISTINCT cat19) as cat19,\n",
    "    COUNT(DISTINCT cat20) as cat20,\n",
    "    COUNT(DISTINCT cat21) as cat21,\n",
    "    COUNT(DISTINCT cat22) as cat22,\n",
    "    COUNT(DISTINCT cat23) as cat23,\n",
    "    COUNT(DISTINCT cat24) as cat24,\n",
    "    COUNT(DISTINCT cat25) as cat25,\n",
    "    COUNT(DISTINCT cat26) as cat26\n",
    "    FROM `{project}.{dataset}.{table_name}`\n",
    "  \"\"\".format(\n",
    "        project=PROJECT_ID,\n",
    "        dataset=DATASET_ID,\n",
    "        table_name='days',\n",
    "    )\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=LOCATION,\n",
    "    )  # API request - starts the query\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "    dictionary = dict((field[0], df[field[0]][0]) for field in df.items())\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "get_vocabulary_size_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from google.cloud import bigquery\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "#TODO: add step to generate corpus and update script to use right table\n",
    "\n",
    "def get_corpus_dict():\n",
    "    table_name = 'days'\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    query = \"\"\"\n",
    "    select\n",
    "    cat_name,\n",
    "    cat_value,\n",
    "    cat_index\n",
    "    from `alekseyv-scalableai-dev.criteo_kaggle.{table_name}_corpus`\n",
    "  \"\"\".format(table_name=table_name)\n",
    "    query_job = client.query(\n",
    "        query,\n",
    "        location=\"US\",\n",
    "    )  # API request - starts the query\n",
    "\n",
    "    df = query_job.to_dataframe()\n",
    "    corpus = dict()\n",
    "    for _, row in df.iterrows():\n",
    "        cat_name = row[0]\n",
    "        cat_value = row[1]\n",
    "        cat_index = row[2]\n",
    "        if not cat_name in corpus:\n",
    "            corpus[cat_name] = dict()\n",
    "        if cat_value is None:\n",
    "            cat_value = ''\n",
    "        corpus[cat_name][cat_value] = cat_index\n",
    "    return corpus\n",
    "\n",
    "def corpus_to_lookuptable(corpus):\n",
    "    lookup_dict = dict()\n",
    "    for key, value in corpus.items():\n",
    "        initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "            list(value.keys()),\n",
    "            list(value.values()),\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64)\n",
    "        # cat_index in corpus starts with 1, reserving 0 for out of vocabulary values\n",
    "        lookup_table = tf.lookup.StaticHashTable(initializer, 0)\n",
    "        lookup_dict[key] = lookup_table\n",
    "    return lookup_dict\n",
    "\n",
    "\n",
    "def get_corpus(embeddings_mode):\n",
    "    if embeddings_mode == EMBEDDINGS_MODE_TYPE.manual or embeddings_mode == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "        return corpus_to_lookuptable(get_corpus_dict())\n",
    "    else:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat1': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f98503684e0>,\n",
       " 'cat2': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffb00>,\n",
       " 'cat3': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff940>,\n",
       " 'cat4': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff780>,\n",
       " 'cat5': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff5c0>,\n",
       " 'cat6': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff400>,\n",
       " 'cat7': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff240>,\n",
       " 'cat8': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff048>,\n",
       " 'cat9': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff208>,\n",
       " 'cat10': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff3c8>,\n",
       " 'cat11': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff588>,\n",
       " 'cat12': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff748>,\n",
       " 'cat13': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bff908>,\n",
       " 'cat14': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffac8>,\n",
       " 'cat15': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffc88>,\n",
       " 'cat16': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824bffe48>,\n",
       " 'cat17': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6ffd0>,\n",
       " 'cat18': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6fe10>,\n",
       " 'cat19': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6fc50>,\n",
       " 'cat20': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6fb00>,\n",
       " 'cat21': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f8d0>,\n",
       " 'cat22': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f710>,\n",
       " 'cat23': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f588>,\n",
       " 'cat24': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f3c8>,\n",
       " 'cat25': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f208>,\n",
       " 'cat26': <tensorflow.python.ops.lookup_ops.StaticHashTable at 0x7f9824c6f080>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "get_corpus(EMBEDDINGS_MODE_TYPE.manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.data.experimental.ops import interleave_ops\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# @tf.function is not really necessary here since function is used in tf.data pipeline\n",
    "# Use print(tf.autograph.to_code(transform_row.python_function)) to see code generated by AutoGraph\n",
    "# Note, in some cases passing enums to autograph transformed funcions have issues.\n",
    "@tf.function\n",
    "def transform_row(row_dict, mean_dict, std_dict, corpus, embeddings_mode):\n",
    "    dict_without_label = dict(row_dict)\n",
    "    label = dict_without_label.pop('label')\n",
    "    for field in CSV_SCHEMA:\n",
    "        if (field.name.startswith('int')):\n",
    "            # use normalized mean value if data is missing (value will be 0)\n",
    "            value = float(dict_without_label[field.name])\n",
    "            dict_without_label[field.name] = \\\n",
    "                (value - mean_dict[field.name]) / std_dict[field.name]\n",
    "        elif field.name.startswith('cat'):\n",
    "            if embeddings_mode == EMBEDDINGS_MODE_TYPE.none.value:\n",
    "                dict_without_label.pop(field.name)\n",
    "            elif embeddings_mode == EMBEDDINGS_MODE_TYPE.manual.value:\n",
    "                cat = dict_without_label[field.name]\n",
    "                if cat is None:\n",
    "                    cat = ''\n",
    "                cat_index = corpus[field.name].lookup(cat)\n",
    "                if cat_index is None:\n",
    "                    tf.print('not found for {}'.format(field.name))\n",
    "                    cat_index = tf.constant(-1)\n",
    "                dict_without_label[field.name] = cat_index\n",
    "    return (dict_without_label, label)\n",
    "\n",
    "def get_bigquery_table_name(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE):\n",
    "    return dataset_type.name + '_' + dataset_size.name\n",
    "\n",
    "def read_bigquery(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE, embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    table_name = get_bigquery_table_name(dataset_size, dataset_type)\n",
    "    (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "    corpus = get_corpus(embedding_mode)\n",
    "    requested_streams_count = 10\n",
    "    tensorflow_io_bigquery_client = BigQueryClient()\n",
    "    read_session = tensorflow_io_bigquery_client.read_session(\n",
    "        \"projects/\" + PROJECT_ID,\n",
    "        PROJECT_ID, table_name, DATASET_ID,\n",
    "        list(field.name for field in CSV_SCHEMA),\n",
    "        list(dtypes.int64 if field.field_type == 'INTEGER'\n",
    "             else dtypes.string for field in CSV_SCHEMA),\n",
    "        requested_streams=requested_streams_count)\n",
    "\n",
    "    # manually sharding output instead of using return read_session.parallel_read_rows()\n",
    "    streams = read_session.get_streams()\n",
    "    # streams_count = len(streams) # does not work for Estimator\n",
    "    streams_count = tf.size(streams)\n",
    "    streams_count64 = tf.cast(streams_count, dtype=tf.int64)\n",
    "    streams_ds = dataset_ops.Dataset.from_tensor_slices(\n",
    "        streams).shuffle(buffer_size=streams_count64)\n",
    "    dataset = streams_ds.interleave(\n",
    "        read_session.read_rows,\n",
    "        cycle_length=streams_count64,\n",
    "        num_parallel_calls=streams_count64)\n",
    "\n",
    "    def transform_row_function(row): \n",
    "        return transform_row(\n",
    "            row, \n",
    "            mean_dict, \n",
    "            std_dict, \n",
    "            corpus, \n",
    "            embedding_mode.value)\n",
    "    \n",
    "    num_parallel_calls = streams_count if tf.version.VERSION[0:3] in ['2.0', '2.1', '2.2'] else streams_count64\n",
    "    transformed_ds = dataset\\\n",
    "        .batch(BATCH_SIZE) \\\n",
    "        .shuffle(50) \\\n",
    "        .map(transform_row_function, num_parallel_calls) \\\n",
    "        .prefetch(50)\n",
    "\n",
    "    # TODO: enable once tf.data.experimental.AutoShardPolicy.OFF is available\n",
    "    # Interleave dataset is not shardable, turning off sharding\n",
    "    # See https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size\n",
    "    # Instead we are shuffling data.\n",
    "    # options = tf.data.Options()\n",
    "    #  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    # return transformed_ds.with_options(options)\n",
    "    return transformed_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "#BATCH_SIZE = 4\n",
    "dataset = read_bigquery(DATASET_SIZE_TYPE.small, DATASET_TYPE.validation, EMBEDDINGS_MODE_TYPE.hashbucket)\n",
    "for row in dataset.take(2):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.data.experimental.ops import interleave_ops\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "\n",
    "import tensorflow as tf\n",
    "import criteo_nbdev.data_import\n",
    "\n",
    "def read_gcs(dataset_size: DATASET_SIZE_TYPE, dataset_type: DATASET_TYPE, embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    file_names = criteo_nbdev.data_import.get_file_names_with_validation_split(dataset_size, dataset_type, 0.2)\n",
    "    num_parallel_calls = max(10, len(file_names))\n",
    "    file_names_ds = dataset_ops.Dataset.from_tensor_slices(file_names).shuffle(buffer_size=20)\n",
    "    record_defaults = list(tf.int32 if field.name == 'label' else tf.constant(0, dtype=tf.int32) if field.name.startswith(\n",
    "        'int') else tf.constant('', dtype=tf.string) for field in CSV_SCHEMA)\n",
    "    dataset = file_names_ds.interleave(\n",
    "        lambda file_name: tf.data.experimental.CsvDataset(\n",
    "            file_name, record_defaults, field_delim='\\t', header=False),\n",
    "        cycle_length=num_parallel_calls,\n",
    "        num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    corpus = get_corpus(embedding_mode)\n",
    "    (mean_dict, std_dict) = get_mean_and_std_dicts()\n",
    "    \n",
    "    def transform_row_function(row_tuple):\n",
    "        row_dict = dict(\n",
    "            zip(list(field.name for field in CSV_SCHEMA), list(row_tuple)))\n",
    "        return transform_row(\n",
    "            row_dict, \n",
    "            mean_dict, \n",
    "            std_dict, \n",
    "            corpus, \n",
    "            embedding_mode.value)\n",
    "    \n",
    "    transofrom_row_gcs_function = lambda *row_tuple: transform_row_function(row_tuple)\n",
    "\n",
    "    transformed_ds = dataset\\\n",
    "        .batch(BATCH_SIZE) \\\n",
    "        .shuffle(50) \\\n",
    "        .map(transofrom_row_gcs_function, num_parallel_calls=num_parallel_calls) \\\n",
    "        .prefetch(50)\n",
    "    return transformed_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "#BATCH_SIZE = 4\n",
    "dataset = read_gcs(DATASET_SIZE_TYPE.small, DATASET_TYPE.validation, EMBEDDINGS_MODE_TYPE.hashbucket)\n",
    "for row in dataset.take(2):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def get_dataset(dataset_source: DATASET_SOURCE_TYPE, \n",
    "                dataset_size: DATASET_SIZE_TYPE, \n",
    "                dataset_type: DATASET_TYPE,\n",
    "                embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    if dataset_source == DATASET_SOURCE_TYPE.gcs:\n",
    "        return read_gcs(dataset_size, dataset_type, embedding_mode)\n",
    "    else:\n",
    "        return read_bigquery(dataset_size, dataset_type, embedding_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "#BATCH_SIZE = 4\n",
    "dataset = get_dataset(DATASET_SOURCE_TYPE.bq, DATASET_SIZE_TYPE.small, DATASET_TYPE.validation, EMBEDDINGS_MODE_TYPE.hashbucket)\n",
    "for row in dataset.take(2):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
