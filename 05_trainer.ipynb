{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "class TrainTimeCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = datetime.datetime.now()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logging.info('\\nepoch train time: (hh:mm:ss.ms) {}'.format(\n",
    "            datetime.datetime.now() - self.epoch_start_time))\n",
    "        if not self.params is None:\n",
    "            if 'steps' in self.params and self.params['steps']:\n",
    "                epoch_milliseconds = (datetime.datetime.now(\n",
    "                ) - self.epoch_start_time).total_seconds() * 1000\n",
    "                logging.info(\n",
    "                    '{} ms/step'.format(epoch_milliseconds / self.params['steps']))\n",
    "                if BATCH_SIZE is not None:\n",
    "                    logging.info('{} microseconds/example'.format(\n",
    "                        1000 * epoch_milliseconds / self.params['steps'] / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_categorical_feature_column_with_hash_bucket(corpus_dict, key):\n",
    "    corpus_size = len(corpus_dict[key])\n",
    "    hash_bucket_size = min(corpus_size, 100000)\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key,\n",
    "        hash_bucket_size,\n",
    "        dtype=tf.dtypes.string\n",
    "    )\n",
    "    logging.info('categorical column %s hash_bucket_size %d',\n",
    "                 key, hash_bucket_size)\n",
    "    return categorical_feature_column\n",
    "\n",
    "\n",
    "def create_categorical_feature_column_with_vocabulary_list(corpus_dict, key):\n",
    "    corpus_size = len(corpus_dict[key])\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key,\n",
    "        list(corpus_dict[key].keys()),\n",
    "        dtype=tf.dtypes.string,\n",
    "        num_oov_buckets=corpus_size\n",
    "    )\n",
    "    logging.info(\n",
    "        'categorical column with vocabular %s corpus_size %d', key, corpus_size)\n",
    "\n",
    "    return categorical_feature_column\n",
    "\n",
    "def create_embedding(vocabulary_size_dict, key, categorical_feature_column):\n",
    "    vocabulary_size = vocabulary_size_dict[key]\n",
    "    if vocabulary_size < 10:\n",
    "        logging.info(\n",
    "            'categorical column %s vocabulary_size %d - creating indicator column', key, vocabulary_size)\n",
    "        return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "    embedding_dimension = int(min(50, math.floor(6 * vocabulary_size**0.25)))\n",
    "    embedding_feature_column = tf.feature_column.embedding_column(\n",
    "        categorical_feature_column,\n",
    "        embedding_dimension)\n",
    "    return embedding_feature_column\n",
    "\n",
    "def create_linear_feature_columns():\n",
    "    return list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32) for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label')\n",
    "\n",
    "def create_categorical_embeddings_feature_columns(corpus_dict, vocabulary_size_dict, embeddings_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    if embeddings_mode == EMBEDDINGS_MODE_TYPE.none:\n",
    "        return []\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.hashbucket:\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_hash_bucket(corpus_dict, key))\n",
    "            for key, _ in corpus_dict.items())\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_vocabulary_list(corpus_dict, key))\n",
    "            for key, _ in corpus_dict.items())\n",
    "    else:\n",
    "        raise ValueError('invalid embedding_mode: {}'.format(embedding_mode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import criteo_nbdev.data_reader\n",
    "\n",
    "def create_feature_columns(embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    corpus_dict = criteo_nbdev.data_reader.get_corpus_dict()\n",
    "    vocabulary_size_dict = criteo_nbdev.data_reader.get_vocabulary_size_dict()\n",
    "    feature_columns = []\n",
    "    feature_columns.extend(create_linear_feature_columns())\n",
    "    feature_columns.extend(\n",
    "        create_categorical_embeddings_feature_columns(corpus_dict, vocabulary_size_dict, embedding_mode))\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_keras_model_sequential():\n",
    "    feature_columns = create_feature_columns(EMBEDDINGS_MODE_TYPE.hashbucket)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns, name=\"feature_layer\")\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    Dropout = tf.keras.layers.Dropout\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            feature_layer,\n",
    "            Dropout(0.3),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "\n",
    "    logging.info('compiling sequential keras model')\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "        # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "        # optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "        optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "        # optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n",
    "        # optimizer=tf.optimizers.Adam(),\n",
    "        #optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.1),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "from gcp_runner.ai_platform_constants import *\n",
    "import criteo_nbdev.data_reader\n",
    "import nbdev.imports\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "def train_and_evaluate_keras_model(\n",
    "    model, \n",
    "    model_dir, \n",
    "    epochs,\n",
    "    dataset_source: DATASET_SOURCE_TYPE,\n",
    "    dataset_size: DATASET_SIZE_TYPE,\n",
    "    embeddings_mode: EMBEDDINGS_MODE_TYPE,\n",
    "    distribution_strategy: DistributionStrategyType):\n",
    "    \n",
    "    log_dir = os.path.join(model_dir, \"logs\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1,\n",
    "        profile_batch=min(epochs, 2))\n",
    "\n",
    "    checkpoints_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    # crashing https://github.com/tensorflow/tensorflow/issues/27688\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "\n",
    "    callbacks = []\n",
    "    train_time_callback = TrainTimeCallback()\n",
    "\n",
    "    if DistributionStrategyType == DistributionStrategyType.TPU_STRATEGY:\n",
    "        # epoch and accuracy constants are not supported when training on TPU.\n",
    "        checkpoints_file_path = checkpoints_dir + \"/epochs_tpu.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        callbacks = [tensorboard_callback,\n",
    "                     checkpoint_callback, train_time_callback]\n",
    "    else:\n",
    "        if embeddings_mode == EMBEDDINGS_MODE_TYPE.manual or distribution_strategy == DistributionStrategyType.MULTI_WORKER_MIRRORED_STRATEGY:\n",
    "            # accuracy fails for adagrad\n",
    "            # for some reason accuracy is not available for EMBEDDINGS_MODE_TYPE.manual\n",
    "            # for some reason accuracy is not available for MultiWorkerMirroredStrategy\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}.hdf5\"\n",
    "        else:\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}-accuracy:{accuracy:.3f}.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        callbacks = [tensorboard_callback, checkpoint_callback, train_time_callback]\n",
    "\n",
    "    verbosity = 1 if nbdev.imports.in_ipython() else 2\n",
    "    logging.info('training keras model')\n",
    "    training_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.training, embeddings_mode).repeat(epochs)\n",
    "    eval_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.validation, embeddings_mode).repeat(epochs)\n",
    "    \n",
    "    # steps_per_epoch and validation_steps are required for MultiWorkerMirroredStrategy\n",
    "    model.fit(\n",
    "        training_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=verbosity,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.training),\n",
    "        validation_data=eval_ds,\n",
    "        validation_steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation))\n",
    "\n",
    "    logging.info(\"done training keras model, evaluating model\")\n",
    "    loss, accuracy = model.evaluate(\n",
    "        eval_ds, \n",
    "        verbose=verbosity, \n",
    "        steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation), \n",
    "        callbacks=[tensorboard_callback])\n",
    "    logging.info(\"Eval - Loss: {}, Accuracy: {}\".format(loss, accuracy))\n",
    "    logging.info(model.summary())\n",
    "    logging.info(\"done evaluating keras model\")\n",
    "    return {'accuracy': accuracy, 'loss': loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)\n",
    "\n",
    "# def train_keras_sequential(strategy, model_dir):\n",
    "#     return train_and_evaluate_keras_model(create_keras_model_sequential(), model_dir)\n",
    "\n",
    "# train_keras_sequential(None, './models/model1')\n",
    "\n",
    "def train_and_evaluate_keras_model_small(args):\n",
    "    print('args:')\n",
    "    print(args)\n",
    "    train_and_evaluate_keras_model(create_keras_model_sequential(), './models/model1', 2, DATASET_SOURCE_TYPE.bq, DATASET_SIZE_TYPE.full, EMBEDDINGS_MODE_TYPE.hashbucket, None)\n",
    "    \n",
    "#train_and_evaluate_keras_model_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as python script:\n",
      "python3 -u -m gcp_runner.entry_point --module-name=criteo_nbdev.trainer --function-name=train_and_evaluate_keras_model_small\n",
      "in gcp_runner entry point\n",
      "running entrypoint function: criteo_nbdev.trainer.train_and_evaluate_keras_model_small\n",
      "\u001b[31m2020-03-31 12:33:01.030897: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\u001b[31m2020-03-31 12:33:10.849933: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\u001b[31m2020-03-31 12:33:10.862852: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8c45999020 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\u001b[0m\n",
      "\u001b[31m2020-03-31 12:33:10.862866: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: HashedCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\u001b[0m\n",
      "Train for 976 steps, validate for 976 steps\n",
      "Epoch 1/2\n",
      "\u001b[31m2020-03-31 12:33:45.727922: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 1 of 50\u001b[0m\n",
      "\u001b[31m2020-03-31 12:33:46.193154: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:199] Shuffle buffer filled.\u001b[0m\n",
      "\u001b[31m2020-03-31 12:33:46.234620: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.\u001b[0m\n",
      "975/976 [============================>.] - ETA: 0s - loss: 0.6433 - accuracy: 0.6271185 \n",
      "Epoch 00001: saving model to ./models/model1/checkpoints/epochs:001-accuracy:0.627.hdf5\n",
      "976/976 [==============================] - 91s 93ms/step - loss: 0.6433 - accuracy: 0.6271 - val_loss: 0.5295 - val_accuracy: 0.7839\n",
      "Epoch 2/2\n",
      "974/976 [============================>.] - ETA: 0s - loss: 0.6110 - accuracy: 0.6668\n",
      "Epoch 00002: saving model to ./models/model1/checkpoints/epochs:002-accuracy:0.667.hdf5\n",
      "976/976 [==============================] - 57s 59ms/step - loss: 0.6110 - accuracy: 0.6667 - val_loss: 0.5014 - val_accuracy: 0.7889\n",
      "976/976 [==============================] - 22s 23ms/step - loss: 0.5016 - accuracy: 0.7887\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "feature_layer (DenseFeatures multiple                  669098    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  541190    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  358202    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  358202    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  599       \n",
      "=================================================================\n",
      "Total params: 1,927,291\n",
      "Trainable params: 1,927,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import gcp_runner.local_runner\n",
    "gcp_runner.local_runner.run_python(train_and_evaluate_keras_model_small, python_binary='python3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: Y: command not found\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "! Y | gcloud auth configure-docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Docker image:\n",
      "docker build -f ./Dockerfile -t gcr.io/alekseyv-scalableai-dev/criteo-nbdev ./\n",
      "Sending build context to Docker daemon  47.73MB\n",
      "Step 1/7 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-1\n",
      " ---> 4f3009408e35\n",
      "Step 2/7 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> ffd9474da319\n",
      "Step 3/7 : RUN git clone https://github.com/vlasenkoalexey/gcp_runner\n",
      " ---> Using cache\n",
      " ---> 5afb9f41adfe\n",
      "Step 4/7 : RUN pip install -e gcp_runner\n",
      " ---> Using cache\n",
      " ---> 510b10a9e9d1\n",
      "Step 5/7 : RUN mkdir /root/models\n",
      " ---> Using cache\n",
      " ---> 3095c1ce545e\n",
      "Step 6/7 : RUN mkdir /root/criteo_nbdev\n",
      " ---> Using cache\n",
      " ---> dc5cf1244117\n",
      "Step 7/7 : COPY criteo_nbdev/* /root/criteo_nbdev/\n",
      " ---> aa76832e26d3\n",
      "Successfully built aa76832e26d3\n",
      "Successfully tagged gcr.io/alekseyv-scalableai-dev/criteo-nbdev:latest\n",
      "Pushing Docker image:\n",
      "docker push gcr.io/alekseyv-scalableai-dev/criteo-nbdev\n",
      "The push refers to repository [gcr.io/alekseyv-scalableai-dev/criteo-nbdev]\n",
      "84a7f41dcd37: Preparing\n",
      "50fe2e90d744: Preparing\n",
      "6dd9cbd264d8: Preparing\n",
      "3beb48e23377: Preparing\n",
      "8c29e4d71687: Preparing\n",
      "41b379bf2eb3: Preparing\n",
      "77db3bd6efb2: Preparing\n",
      "07fd8d677a40: Preparing\n",
      "992939e921d8: Preparing\n",
      "a304fb96c494: Preparing\n",
      "431f13f6088f: Preparing\n",
      "f63b09c90bb4: Preparing\n",
      "eac1f876522f: Preparing\n",
      "391d3bae9f0a: Preparing\n",
      "0d69a3a385f0: Preparing\n",
      "dab306330749: Preparing\n",
      "a6c05cf3b0f4: Preparing\n",
      "1c0e7affc630: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "41b379bf2eb3: Waiting\n",
      "431f13f6088f: Waiting\n",
      "f63b09c90bb4: Waiting\n",
      "77db3bd6efb2: Waiting\n",
      "eac1f876522f: Waiting\n",
      "07fd8d677a40: Waiting\n",
      "992939e921d8: Waiting\n",
      "a304fb96c494: Waiting\n",
      "391d3bae9f0a: Waiting\n",
      "0d69a3a385f0: Waiting\n",
      "dab306330749: Waiting\n",
      "a6c05cf3b0f4: Waiting\n",
      "1c0e7affc630: Waiting\n",
      "1852b2300972: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "6dd9cbd264d8: Layer already exists\n",
      "3beb48e23377: Layer already exists\n",
      "50fe2e90d744: Layer already exists\n",
      "8c29e4d71687: Layer already exists\n",
      "41b379bf2eb3: Layer already exists\n",
      "77db3bd6efb2: Layer already exists\n",
      "992939e921d8: Layer already exists\n",
      "a304fb96c494: Layer already exists\n",
      "431f13f6088f: Layer already exists\n",
      "f63b09c90bb4: Layer already exists\n",
      "eac1f876522f: Layer already exists\n",
      "391d3bae9f0a: Layer already exists\n",
      "dab306330749: Layer already exists\n",
      "84a7f41dcd37: Pushed\n",
      "1852b2300972: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b993d105dced>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_and_evaluate_keras_model_small\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'gcr.io/alekseyv-scalableai-dev/criteo-nbdev'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     build_docker_file='./Dockerfile')\n\u001b[0m",
      "\u001b[0;32m~/vlasenkoalexey/gcp_runner/gcp_runner/local_runner.py\u001b[0m in \u001b[0;36mrun_docker\u001b[0;34m(func, image_uri, build_docker_file, docker_args, dry_run, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_docker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_docker_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocker_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbuild_docker_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_and_push_docker_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_docker_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vlasenkoalexey/gcp_runner/gcp_runner/core.py\u001b[0m in \u001b[0;36mbuild_and_push_docker_image\u001b[0;34m(docker_file_path, image_uri, dry_run)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpush_docker_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpush_docker_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vlasenkoalexey/gcp_runner/gcp_runner/core.py\u001b[0m in \u001b[0;36mrun_process\u001b[0;34m(args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m#for _ in range(2):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'stderr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gcp_runner.local_runner\n",
    "gcp_runner.local_runner.run_docker(\n",
    "    train_and_evaluate_keras_model_small,\n",
    "    'gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "    build_docker_file='./Dockerfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running training job using package on Google Cloud Platform AI:\n",
      "gcloud ai-platform jobs submit training ai_platform_runner_train_package_20200331_133010 \\ \n",
      " --runtime-version=2.1 \\ \n",
      " --python-version=3.7 \\ \n",
      " --stream-logs \\ \n",
      " --module-name=criteo_nbdev.entry_point \\ \n",
      " --package-path=/Users/alekseyv/vlasenkoalexey/criteo_nbdev/criteo_nbdev \\ \n",
      " --scale-tier=basic \\ \n",
      " --region=us-west1 \\ \n",
      " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir \\ \n",
      " -- \\ \n",
      " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir \\ \n",
      " --module-name=criteo_nbdev.trainer \\ \n",
      " --function-name=train_and_evaluate_keras_model_small\n",
      "\u001b[31mJob [ai_platform_runner_train_package_20200331_133010] submitted successfully.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:30:13 -0700\tservice\t\tValidating job requirements...\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:30:13 -0700\tservice\t\tJob creation request has been successfully validated.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:30:13 -0700\tservice\t\tWaiting for job to be provisioned.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:30:13 -0700\tservice\t\tJob ai_platform_runner_train_package_20200331_133010 is queued.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:30:16 -0700\tservice\t\tWaiting for training program to start.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:30:53 -0700\tmaster-replica-0\t\tRunning task with arguments: --cluster={\"chief\": [\"127.0.0.1:2222\"]} --task={\"type\": \"chief\", \"index\": 0} --job={  \"package_uris\": [\"gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/packages/be39964c42a6ab2b79473b545ce30997f9b496842f87d893aa9da66abe305cd6/criteo_nbdev-0.0.1.tar.gz\"],  \"python_module\": \"criteo_nbdev.entry_point\",  \"args\": [\"--job-dir\\u003dgs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir\", \"--module-name\\u003dcriteo_nbdev.trainer\", \"--function-name\\u003dtrain_and_evaluate_keras_model_small\"],  \"region\": \"us-west1\",  \"runtime_version\": \"2.1\",  \"job_dir\": \"gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir\",  \"run_on_raw_vm\": true,  \"python_version\": \"3.7\"}\u001b[0m\n",
      "\u001b[31mWARNING\t2020-03-31 13:31:10 -0700\tmaster-replica-0\t\tFrom /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mWARNING\t2020-03-31 13:31:10 -0700\tmaster-replica-0\t\tInstructions for updating:\u001b[0m\n",
      "\u001b[31mWARNING\t2020-03-31 13:31:10 -0700\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:29 -0700\tmaster-replica-0\t\tRunning module criteo_nbdev.entry_point.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:29 -0700\tmaster-replica-0\t\tDownloading the package: gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/packages/be39964c42a6ab2b79473b545ce30997f9b496842f87d893aa9da66abe305cd6/criteo_nbdev-0.0.1.tar.gz\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:29 -0700\tmaster-replica-0\t\tRunning command: gsutil -q cp gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/packages/be39964c42a6ab2b79473b545ce30997f9b496842f87d893aa9da66abe305cd6/criteo_nbdev-0.0.1.tar.gz criteo_nbdev-0.0.1.tar.gz\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:31 -0700\tmaster-replica-0\t\tInstalling the package: gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/packages/be39964c42a6ab2b79473b545ce30997f9b496842f87d893aa9da66abe305cd6/criteo_nbdev-0.0.1.tar.gz\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:31 -0700\tmaster-replica-0\t\tRunning command: pip3 install --user --upgrade --force-reinstall --no-deps criteo_nbdev-0.0.1.tar.gz\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tProcessing ./criteo_nbdev-0.0.1.tar.gz\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tBuilding wheels for collected packages: criteo-nbdev\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\t  Building wheel for criteo-nbdev (setup.py): started\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\t  Building wheel for criteo-nbdev (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\t  Created wheel for criteo-nbdev: filename=criteo_nbdev-0.0.1-py3-none-any.whl size=16447 sha256=94fdd30e9096375b558f26a88e2de57473a07996f0e13ee34426c4e74f7e0c5f\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/4d/84/ab/b580fb005e13115775ef376a3039445452d46d5fdb272c03ff\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tSuccessfully built criteo-nbdev\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tInstalling collected packages: criteo-nbdev\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:32 -0700\tmaster-replica-0\t\tSuccessfully installed criteo-nbdev-0.0.1\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:33 -0700\tmaster-replica-0\t\tRunning command: pip3 install --user criteo_nbdev-0.0.1.tar.gz\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:33 -0700\tmaster-replica-0\t\tProcessing ./criteo_nbdev-0.0.1.tar.gz\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:33 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:33 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:33 -0700\tmaster-replica-0\t\tBuilding wheels for collected packages: criteo-nbdev\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:33 -0700\tmaster-replica-0\t\t  Building wheel for criteo-nbdev (setup.py): started\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tGenerating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t  Building wheel for criteo-nbdev (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t  Created wheel for criteo-nbdev: filename=criteo_nbdev-0.0.1-py3-none-any.whl size=16447 sha256=381f8cbd092b01d5eaabeba37aa44c0eb0b74cfe4770338c84d134ef755b1e4f\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t  Stored in directory: /root/.cache/pip/wheels/4d/84/ab/b580fb005e13115775ef376a3039445452d46d5fdb272c03ff\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tSuccessfully built criteo-nbdev\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tInstalling collected packages: criteo-nbdev\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t  Attempting uninstall: criteo-nbdev\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t    Found existing installation: criteo-nbdev 0.0.1\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t    Uninstalling criteo-nbdev-0.0.1:\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t      Successfully uninstalled criteo-nbdev-0.0.1\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tSuccessfully installed criteo-nbdev-0.0.1\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tRunning command: python3 -m criteo_nbdev.entry_point --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir --module-name=criteo_nbdev.trainer --function-name=train_and_evaluate_keras_model_small --job-dir gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tTraceback (most recent call last):\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t    exec(code, run_globals)\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t  File \"/root/.local/lib/python3.7/site-packages/criteo_nbdev/entry_point.py\", line 6, in <module>\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\t    import gcp_runner.entry_point\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tModuleNotFoundError: No module named 'gcp_runner'\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tCommand '['python3', '-m', 'criteo_nbdev.entry_point', '--job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir', '--module-name=criteo_nbdev.trainer', '--function-name=train_and_evaluate_keras_model_small', '--job-dir', 'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir']' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tModule completed; cleaning up.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 13:31:34 -0700\tmaster-replica-0\t\tClean up finished.\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\tThe replica master 0 exited with a non-zero status of 1. \u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\tTraceback (most recent call last):\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\t  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\t    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\t  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\t    exec(code, run_globals)\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\t  File \"/root/.local/lib/python3.7/site-packages/criteo_nbdev/entry_point.py\", line 6, in <module>\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\t    import gcp_runner.entry_point\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\tModuleNotFoundError: No module named 'gcp_runner'\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\t\u001b[0m\n",
      "\u001b[31mERROR\t2020-03-31 13:31:58 -0700\tservice\t\tTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=951953070105&resource=ml_job%2Fjob_id%2Fai_platform_runner_train_package_20200331_133010&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22ai_platform_runner_train_package_20200331_133010%22\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-97417f3048ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0;34m'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0muse_chief_in_tf_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m      region='us-west1')\n\u001b[0m",
      "\u001b[0;32m~/vlasenkoalexey/gcp_runner/gcp_runner/ai_platform_runner.py\u001b[0m in \u001b[0;36mrun_package\u001b[0;34m(func, job_dir, job_name, runtime_version, python_version, dry_run, region, scale_tier, master_machine_type, master_image_uri, master_accelerator_type, master_accelerator_count, parameter_machine_type, parameter_machine_count, parameter_image_uri, parameter_accelerator_type, parameter_accelerator_count, worker_machine_type, worker_machine_count, worker_image_uri, work_accelerator_type, work_accelerator_count, use_chief_in_tf_config, distribution_strategy_type, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/vlasenkoalexey/gcp_runner/gcp_runner/core.py\u001b[0m in \u001b[0;36mrun_process\u001b[0;34m(args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m#for _ in range(2):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'stderr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gcp_runner.ai_platform_runner\n",
    "gcp_runner.ai_platform_runner.run_package(\n",
    "     train_and_evaluate_keras_model_small,     \n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir',\n",
    "     use_chief_in_tf_config=None,\n",
    "     region='us-west1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['criteo_nbdev']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import setuptools\n",
    "\n",
    "setuptools.find_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
