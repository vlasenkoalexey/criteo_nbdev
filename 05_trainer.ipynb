{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "class TrainTimeCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = datetime.datetime.now()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logging.info('\\nepoch train time: (hh:mm:ss.ms) {}'.format(\n",
    "            datetime.datetime.now() - self.epoch_start_time))\n",
    "        if not self.params is None:\n",
    "            if 'steps' in self.params and self.params['steps']:\n",
    "                epoch_milliseconds = (datetime.datetime.now(\n",
    "                ) - self.epoch_start_time).total_seconds() * 1000\n",
    "                logging.info(\n",
    "                    '{} ms/step'.format(epoch_milliseconds / self.params['steps']))\n",
    "                if BATCH_SIZE is not None:\n",
    "                    logging.info('{} microseconds/example'.format(\n",
    "                        1000 * epoch_milliseconds / self.params['steps'] / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_categorical_feature_column_with_hash_bucket(corpus_dict, key):\n",
    "    corpus_size = len(corpus_dict[key])\n",
    "    hash_bucket_size = min(corpus_size, 100000)\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key,\n",
    "        hash_bucket_size,\n",
    "        dtype=tf.dtypes.string\n",
    "    )\n",
    "    logging.info('categorical column %s hash_bucket_size %d',\n",
    "                 key, hash_bucket_size)\n",
    "    return categorical_feature_column\n",
    "\n",
    "\n",
    "def create_categorical_feature_column_with_vocabulary_list(corpus_dict, key):\n",
    "    corpus_size = len(corpus_dict[key])\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key,\n",
    "        list(corpus_dict[key].keys()),\n",
    "        dtype=tf.dtypes.string,\n",
    "        num_oov_buckets=corpus_size\n",
    "    )\n",
    "    logging.info(\n",
    "        'categorical column with vocabular %s corpus_size %d', key, corpus_size)\n",
    "\n",
    "    return categorical_feature_column\n",
    "\n",
    "def create_embedding(vocabulary_size_dict, key, categorical_feature_column):\n",
    "    vocabulary_size = vocabulary_size_dict[key]\n",
    "    if vocabulary_size < 10:\n",
    "        logging.info(\n",
    "            'categorical column %s vocabulary_size %d - creating indicator column', key, vocabulary_size)\n",
    "        return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "    embedding_dimension = int(min(50, math.floor(6 * vocabulary_size**0.25)))\n",
    "    embedding_feature_column = tf.feature_column.embedding_column(\n",
    "        categorical_feature_column,\n",
    "        embedding_dimension)\n",
    "    return embedding_feature_column\n",
    "\n",
    "def create_linear_feature_columns():\n",
    "    return list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32) for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label')\n",
    "\n",
    "def create_categorical_embeddings_feature_columns(corpus_dict, vocabulary_size_dict, embeddings_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    if embeddings_mode == EMBEDDINGS_MODE_TYPE.none:\n",
    "        return []\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.hashbucket:\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_hash_bucket(corpus_dict, key))\n",
    "            for key, _ in corpus_dict.items())\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_vocabulary_list(corpus_dict, key))\n",
    "            for key, _ in corpus_dict.items())\n",
    "    else:\n",
    "        raise ValueError('invalid embedding_mode: {}'.format(embedding_mode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import criteo_nbdev.data_reader\n",
    "\n",
    "def create_feature_columns(embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    corpus_dict = criteo_nbdev.data_reader.get_corpus_dict()\n",
    "    vocabulary_size_dict = criteo_nbdev.data_reader.get_vocabulary_size_dict()\n",
    "    feature_columns = []\n",
    "    feature_columns.extend(create_linear_feature_columns())\n",
    "    feature_columns.extend(\n",
    "        create_categorical_embeddings_feature_columns(corpus_dict, vocabulary_size_dict, embedding_mode))\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_keras_model_sequential():\n",
    "    feature_columns = create_feature_columns(EMBEDDINGS_MODE_TYPE.hashbucket)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns, name=\"feature_layer\")\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    Dropout = tf.keras.layers.Dropout\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            feature_layer,\n",
    "            Dropout(0.3),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "\n",
    "    logging.info('compiling sequential keras model')\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "        # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "        # optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "        optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "        # optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n",
    "        # optimizer=tf.optimizers.Adam(),\n",
    "        #optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.1),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "from gcp_runner.ai_platform_constants import *\n",
    "import criteo_nbdev.data_reader\n",
    "import nbdev.imports\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "def train_and_evaluate_keras_model(\n",
    "    model, \n",
    "    model_dir, \n",
    "    epochs,\n",
    "    dataset_source: DATASET_SOURCE_TYPE,\n",
    "    dataset_size: DATASET_SIZE_TYPE,\n",
    "    embeddings_mode: EMBEDDINGS_MODE_TYPE,\n",
    "    distribution_strategy: DistributionStrategyType):\n",
    "    \n",
    "    log_dir = os.path.join(model_dir, \"logs\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1,\n",
    "        profile_batch=min(epochs, 2))\n",
    "\n",
    "    checkpoints_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    # crashing https://github.com/tensorflow/tensorflow/issues/27688\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "\n",
    "    callbacks = []\n",
    "    train_time_callback = TrainTimeCallback()\n",
    "\n",
    "    if DistributionStrategyType == DistributionStrategyType.TPU_STRATEGY:\n",
    "        # epoch and accuracy constants are not supported when training on TPU.\n",
    "        checkpoints_file_path = checkpoints_dir + \"/epochs_tpu.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        callbacks = [tensorboard_callback,\n",
    "                     checkpoint_callback, train_time_callback]\n",
    "    else:\n",
    "        if embeddings_mode == EMBEDDINGS_MODE_TYPE.manual or distribution_strategy == DistributionStrategyType.MULTI_WORKER_MIRRORED_STRATEGY:\n",
    "            # accuracy fails for adagrad\n",
    "            # for some reason accuracy is not available for EMBEDDINGS_MODE_TYPE.manual\n",
    "            # for some reason accuracy is not available for MultiWorkerMirroredStrategy\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}.hdf5\"\n",
    "        else:\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}-accuracy:{accuracy:.3f}.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        callbacks = [tensorboard_callback, checkpoint_callback, train_time_callback]\n",
    "\n",
    "    verbosity = 1 if nbdev.imports.in_ipython() else 2\n",
    "    logging.info('training keras model')\n",
    "    training_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.training, embeddings_mode).repeat(epochs)\n",
    "    eval_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.validation, embeddings_mode).repeat(epochs)\n",
    "    \n",
    "    # steps_per_epoch and validation_steps are required for MultiWorkerMirroredStrategy\n",
    "    model.fit(\n",
    "        training_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=verbosity,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.training),\n",
    "        validation_data=eval_ds,\n",
    "        validation_steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation))\n",
    "\n",
    "    logging.info(\"done training keras model, evaluating model\")\n",
    "    loss, accuracy = model.evaluate(\n",
    "        eval_ds, \n",
    "        verbose=verbosity, \n",
    "        steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation), \n",
    "        callbacks=[tensorboard_callback])\n",
    "    logging.info(\"Eval - Loss: {}, Accuracy: {}\".format(loss, accuracy))\n",
    "    logging.info(model.summary())\n",
    "    logging.info(\"done evaluating keras model\")\n",
    "    return {'accuracy': accuracy, 'loss': loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.local_runner\n",
    "gcp_runner.local_runner.run_python(train_and_evaluate_keras_model_small, python_binary='python3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)\n",
    "\n",
    "# def train_keras_sequential(strategy, model_dir):\n",
    "#     return train_and_evaluate_keras_model(create_keras_model_sequential(), model_dir)\n",
    "\n",
    "# train_keras_sequential(None, './models/model1')\n",
    "\n",
    "def train_and_evaluate_keras_model_small(distribution_strategy=None, **kwargs):\n",
    "    print('distribution_strategy:')\n",
    "    print(distribution_strategy)\n",
    "    print('kwargs:')\n",
    "    print(kwargs)\n",
    "#     print('args:')7\n",
    "#     print(args)\n",
    "    #train_and_evaluate_keras_model(create_keras_model_sequential(), './models/model1', 2, DATASET_SOURCE_TYPE.bq, DATASET_SIZE_TYPE.full, EMBEDDINGS_MODE_TYPE.hashbucket, None)\n",
    "    \n",
    "#train_and_evaluate_keras_model_small()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution_strategy:\n",
      "d\n",
      "kwargs:\n",
      "{'other_arg': 'o'}\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_keras_model_small(**{'distribution_strategy':'d', 'other_arg': 'o'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python',\n",
       " '-u',\n",
       " '-m',\n",
       " 'gcp_runner.entry_point',\n",
       " '--module-name=criteo_nbdev.trainer',\n",
       " '--function-name=train_and_evaluate_keras_model_small',\n",
       " '--distribution-strategy=tf.distribute.OneDeviceStrategy',\n",
       " '--arg-1=arg_1_value']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcp_runner.core\n",
    "from gcp_runner.ai_platform_constants import DistributionStrategyType\n",
    "\n",
    "\n",
    "gcp_runner.core.get_run_python_args(train_and_evaluate_keras_model_small, distribution_strategy=DistributionStrategyType.ONE_DEVICE_STRATEGY, arg_1='arg_1_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Docker image:\n",
      "docker build -f ./Dockerfile -t gcr.io/alekseyv-scalableai-dev/criteo-nbdev ./\n",
      "Sending build context to Docker daemon  2.922MB\n",
      "Step 1/12 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-1\n",
      " ---> 4f3009408e35\n",
      "Step 2/12 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> ffd9474da319\n",
      "Step 3/12 : ENV PROJECT_ID=alekseyv-scalableai-dev\n",
      " ---> Using cache\n",
      " ---> e65292b3e3c8\n",
      "Step 4/12 : ENV GOOGLE_APPLICATION_CREDENTIALS=/root/service_account_key.json\n",
      " ---> Using cache\n",
      " ---> 41bad1b2743d\n",
      "Step 5/12 : COPY service_account_key.json /root/\n",
      " ---> Using cache\n",
      " ---> cf7f34d90229\n",
      "Step 6/12 : RUN pip install nbdev\n",
      " ---> Using cache\n",
      " ---> b1d87c2fcf23\n",
      "Step 7/12 : ADD \"https://www.random.org/cgi-bin/randbyte?nbytes=10&format=h\" skipcache\n",
      "\n",
      "\n",
      " ---> 5269cc28acd2\n",
      "Step 8/12 : RUN git clone https://github.com/vlasenkoalexey/gcp_runner\n",
      " ---> Running in 6d3c2a6ee67e\n",
      "\u001b[91mCloning into 'gcp_runner'...\n",
      "\u001b[0mRemoving intermediate container 6d3c2a6ee67e\n",
      " ---> 71cacad26f01\n",
      "Step 9/12 : RUN pip install -e gcp_runner\n",
      " ---> Running in 67355d863830\n",
      "Obtaining file:///root/gcp_runner\n",
      "Installing collected packages: gcp-runner\n",
      "  Running setup.py develop for gcp-runner\n",
      "Successfully installed gcp-runner\n",
      "Removing intermediate container 67355d863830\n",
      " ---> 3f9716195ee4\n",
      "Step 10/12 : RUN mkdir /root/models\n",
      " ---> Running in 2880113f4909\n",
      "Removing intermediate container 2880113f4909\n",
      " ---> 0587f09a5347\n",
      "Step 11/12 : RUN mkdir /root/criteo_nbdev\n",
      " ---> Running in e97b1bcba57b\n",
      "Removing intermediate container e97b1bcba57b\n",
      " ---> d228abfce96e\n",
      "Step 12/12 : COPY criteo_nbdev/* /root/criteo_nbdev/\n",
      " ---> e952d25d8cd7\n",
      "Successfully built e952d25d8cd7\n",
      "Successfully tagged gcr.io/alekseyv-scalableai-dev/criteo-nbdev:latest\n",
      "Running in Docker container:\n",
      "docker run -v /Users/alekseyv/vlasenkoalexey/criteo_nbdev/criteo_nbdev:/criteo_nbdev gcr.io/alekseyv-scalableai-dev/criteo-nbdev python -u -m gcp_runner.entry_point --module-name=criteo_nbdev.trainer --function-name=train_and_evaluate_keras_model_small --distribution-strategy=tf.distribute.OneDeviceStrategy --arg-1=arg_1_value\n",
      "in gcp_runner entry point\n",
      "running entrypoint function: criteo_nbdev.trainer.train_and_evaluate_keras_model_small\n",
      "\u001b[31m2020-04-01 01:32:56.773419: I tensorflow_io/core/kernels/cpu_check.cc:127] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "additional args: ['--arg-1=arg_1_value']\n",
      "\u001b[31m/root/gcp_runner/gcp_runner/entry_point.py:74: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\u001b[0m\n",
      "\u001b[31m  args_spec = inspect.getargspec(func)\u001b[0m\n",
      "ArgSpec(args=['distribution_strategy'], varargs=None, keywords='kwargs', defaults=(None,))\n",
      "distribution_strategy:\n",
      "<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7fa9bb006490>\n",
      "kwargs:\n",
      "{'arg_1': 'arg_1_value'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcp_runner.local_runner\n",
    "gcp_runner.local_runner.run_docker(\n",
    "    train_and_evaluate_keras_model_small,\n",
    "    'gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "    build_docker_file='./Dockerfile', distribution_strategy=DistributionStrategyType.ONE_DEVICE_STRATEGY, arg_1='arg_1_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running training job using package on Google Cloud Platform AI:\n",
      "gcloud ai-platform jobs submit training ai_platform_runner_train_package_20200331_183505 \\ \n",
      " --runtime-version=2.1 \\ \n",
      " --python-version=3.7 \\ \n",
      " --stream-logs \\ \n",
      " --module-name=criteo_nbdev.entry_point \\ \n",
      " --package-path=/Users/alekseyv/vlasenkoalexey/criteo_nbdev/criteo_nbdev \\ \n",
      " --scale-tier=basic \\ \n",
      " --region=us-west1 \\ \n",
      " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir \\ \n",
      " -- \\ \n",
      " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir \\ \n",
      " --module-name=criteo_nbdev.trainer \\ \n",
      " --function-name=train_and_evaluate_keras_model_small\n",
      "\u001b[31mERROR: (gcloud.ai-platform.jobs.submit.training) Packaging of user Python code failed with message:\u001b[0m\n",
      "\u001b[31m\u001b[0m\n",
      "\u001b[31merror in criteo_nbdev setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at \"',pandas='\"\u001b[0m\n",
      "\u001b[31m\u001b[0m\n",
      "\u001b[31m\u001b[0m\n",
      "\u001b[31mTry manually building your Python code by running:\u001b[0m\n",
      "\u001b[31m  $ python setup.py sdist\u001b[0m\n",
      "\u001b[31mand providing the output via the `--packages` flag (for example, `--packages dist/package.tar.gz,dist/package2.whl)`\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcp_runner.ai_platform_runner\n",
    "gcp_runner.ai_platform_runner.run_package(\n",
    "     train_and_evaluate_keras_model_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir',\n",
    "     use_chief_in_tf_config=None,\n",
    "     region='us-west1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Docker image:\n",
      "docker build -f ./Dockerfile -t gcr.io/alekseyv-scalableai-dev/criteo-nbdev ./\n",
      "Sending build context to Docker daemon  2.922MB\n",
      "Step 1/12 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-1\n",
      " ---> 4f3009408e35\n",
      "Step 2/12 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> ffd9474da319\n",
      "Step 3/12 : ENV PROJECT_ID=alekseyv-scalableai-dev\n",
      " ---> Using cache\n",
      " ---> e65292b3e3c8\n",
      "Step 4/12 : ENV GOOGLE_APPLICATION_CREDENTIALS=/root/service_account_key.json\n",
      " ---> Using cache\n",
      " ---> 41bad1b2743d\n",
      "Step 5/12 : COPY service_account_key.json /root/\n",
      " ---> Using cache\n",
      " ---> cf7f34d90229\n",
      "Step 6/12 : RUN pip install nbdev\n",
      " ---> Using cache\n",
      " ---> b1d87c2fcf23\n",
      "Step 7/12 : ADD \"https://www.random.org/cgi-bin/randbyte?nbytes=10&format=h\" skipcache\n",
      "\n",
      "\n",
      " ---> 90870c6dfd00\n",
      "Step 8/12 : RUN git clone https://github.com/vlasenkoalexey/gcp_runner\n",
      " ---> Running in 12b3e07e4ab5\n",
      "\u001b[91mCloning into 'gcp_runner'...\n",
      "\u001b[0mRemoving intermediate container 12b3e07e4ab5\n",
      " ---> b157e9b59059\n",
      "Step 9/12 : RUN pip install -e gcp_runner\n",
      " ---> Running in 7a3708fc505d\n",
      "Obtaining file:///root/gcp_runner\n",
      "Installing collected packages: gcp-runner\n",
      "  Running setup.py develop for gcp-runner\n",
      "Successfully installed gcp-runner\n",
      "Removing intermediate container 7a3708fc505d\n",
      " ---> a20dc1b60c24\n",
      "Step 10/12 : RUN mkdir /root/models\n",
      " ---> Running in 1a4ef8a077fc\n",
      "Removing intermediate container 1a4ef8a077fc\n",
      " ---> cda4e81054a5\n",
      "Step 11/12 : RUN mkdir /root/criteo_nbdev\n",
      " ---> Running in 80a10d070abb\n",
      "Removing intermediate container 80a10d070abb\n",
      " ---> 94690e6f581f\n",
      "Step 12/12 : COPY criteo_nbdev/* /root/criteo_nbdev/\n",
      " ---> 3c1b40a0628a\n",
      "Successfully built 3c1b40a0628a\n",
      "Successfully tagged gcr.io/alekseyv-scalableai-dev/criteo-nbdev:latest\n",
      "Pushing Docker image:\n",
      "docker push gcr.io/alekseyv-scalableai-dev/criteo-nbdev\n",
      "The push refers to repository [gcr.io/alekseyv-scalableai-dev/criteo-nbdev]\n",
      "ef7ef721d20c: Preparing\n",
      "4df940a1c3ac: Preparing\n",
      "7bad79c435f3: Preparing\n",
      "a8248e2a2e2a: Preparing\n",
      "671ab06ca18c: Preparing\n",
      "94b0b9f9bb18: Preparing\n",
      "24cb2d3fea08: Preparing\n",
      "5c382f491923: Preparing\n",
      "41b379bf2eb3: Preparing\n",
      "77db3bd6efb2: Preparing\n",
      "07fd8d677a40: Preparing\n",
      "992939e921d8: Preparing\n",
      "a304fb96c494: Preparing\n",
      "431f13f6088f: Preparing\n",
      "f63b09c90bb4: Preparing\n",
      "eac1f876522f: Preparing\n",
      "391d3bae9f0a: Preparing\n",
      "0d69a3a385f0: Preparing\n",
      "dab306330749: Preparing\n",
      "a6c05cf3b0f4: Preparing\n",
      "1c0e7affc630: Preparing\n",
      "1852b2300972: Preparing\n",
      "03c9b9f537a4: Preparing\n",
      "8c98131d2d1d: Preparing\n",
      "cc4590d6a718: Preparing\n",
      "77db3bd6efb2: Waiting\n",
      "07fd8d677a40: Waiting\n",
      "992939e921d8: Waiting\n",
      "a304fb96c494: Waiting\n",
      "431f13f6088f: Waiting\n",
      "f63b09c90bb4: Waiting\n",
      "eac1f876522f: Waiting\n",
      "391d3bae9f0a: Waiting\n",
      "0d69a3a385f0: Waiting\n",
      "dab306330749: Waiting\n",
      "94b0b9f9bb18: Waiting\n",
      "24cb2d3fea08: Waiting\n",
      "1c0e7affc630: Waiting\n",
      "1852b2300972: Waiting\n",
      "03c9b9f537a4: Waiting\n",
      "8c98131d2d1d: Waiting\n",
      "5c382f491923: Waiting\n",
      "41b379bf2eb3: Waiting\n",
      "cc4590d6a718: Waiting\n",
      "a6c05cf3b0f4: Waiting\n",
      "7bad79c435f3: Pushed\n",
      "ef7ef721d20c: Pushed\n",
      "a8248e2a2e2a: Pushed\n",
      "24cb2d3fea08: Layer already exists\n",
      "4df940a1c3ac: Pushed\n",
      "5c382f491923: Layer already exists\n",
      "41b379bf2eb3: Layer already exists\n",
      "77db3bd6efb2: Layer already exists\n",
      "992939e921d8: Layer already exists\n",
      "a304fb96c494: Layer already exists\n",
      "07fd8d677a40: Layer already exists\n",
      "431f13f6088f: Layer already exists\n",
      "f63b09c90bb4: Layer already exists\n",
      "eac1f876522f: Layer already exists\n",
      "391d3bae9f0a: Layer already exists\n",
      "0d69a3a385f0: Layer already exists\n",
      "dab306330749: Layer already exists\n",
      "a6c05cf3b0f4: Layer already exists\n",
      "1c0e7affc630: Layer already exists\n",
      "1852b2300972: Layer already exists\n",
      "03c9b9f537a4: Layer already exists\n",
      "cc4590d6a718: Layer already exists\n",
      "8c98131d2d1d: Layer already exists\n",
      "94b0b9f9bb18: Pushed\n",
      "671ab06ca18c: Pushed\n",
      "latest: digest: sha256:7b317ecedfb7c480d90aa60df12fe5a64d4c9d91b097664a024918019e634884 size: 5544\n",
      "['--use-chief-in-tf-config=True', '--master-image-uri=gcr.io/alekseyv-scalableai-dev/criteo-nbdev', '--scale-tier=basic', '--region=us-west1', '--job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir']\n",
      "gcr.io/alekseyv-scalableai-dev/criteo-nbdev\n",
      "running training job using Docker image Google Cloud Platform AI:\n",
      "gcloud ai-platform jobs submit training ai_platform_runner_train_docker_20200331_183719 \\ \n",
      " --stream-logs \\ \n",
      " --use-chief-in-tf-config=True \\ \n",
      " --master-image-uri=gcr.io/alekseyv-scalableai-dev/criteo-nbdev \\ \n",
      " --scale-tier=basic \\ \n",
      " --region=us-west1 \\ \n",
      " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir \\ \n",
      " -- python -u -m gcp_runner.entry_point \\ \n",
      " --module-name=criteo_nbdev.trainer \\ \n",
      " --function-name=train_and_evaluate_keras_model_small \\ \n",
      " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir\n",
      "\u001b[31mJob [ai_platform_runner_train_docker_20200331_183719] submitted successfully.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 18:37:43 -0700\tservice\t\tValidating job requirements...\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 18:37:45 -0700\tservice\t\tJob creation request has been successfully validated.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 18:37:45 -0700\tservice\t\tJob ai_platform_runner_train_docker_20200331_183719 is queued.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 18:37:45 -0700\tservice\t\tWaiting for job to be provisioned.\u001b[0m\n",
      "\u001b[31mINFO\t2020-03-31 18:37:48 -0700\tservice\t\tWaiting for training program to start.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gcp_runner.ai_platform_runner\n",
    "gcp_runner.ai_platform_runner.run_docker_image(\n",
    "     train_and_evaluate_keras_model_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir',\n",
    "     master_image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     region='us-west1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
