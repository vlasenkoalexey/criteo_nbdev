{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "> Code to do model training using Keras and Estimator APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from criteo_nbdev.constants import *\n",
    "from gcp_runner.ai_platform_constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class TrainTimeCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = datetime.datetime.now()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logging.info('\\nepoch train time: (hh:mm:ss.ms) {}'.format(\n",
    "            datetime.datetime.now() - self.epoch_start_time))\n",
    "        if not self.params is None:\n",
    "            if 'steps' in self.params and self.params['steps']:\n",
    "                epoch_milliseconds = (datetime.datetime.now(\n",
    "                ) - self.epoch_start_time).total_seconds() * 1000\n",
    "                logging.info(\n",
    "                    '{} ms/step'.format(epoch_milliseconds / self.params['steps']))\n",
    "                if BATCH_SIZE is not None:\n",
    "                    logging.info('{} microseconds/example'.format(\n",
    "                        1000 * epoch_milliseconds / self.params['steps'] / BATCH_SIZE))\n",
    "                    \n",
    "\n",
    "\n",
    "class PlotLossesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_categorical_feature_column_with_hash_bucket(vocabulary_size_dict, key):\n",
    "    corpus_size = vocabulary_size_dict[key]\n",
    "    hash_bucket_size = min(corpus_size, 100000)\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key,\n",
    "        hash_bucket_size,\n",
    "        dtype=tf.dtypes.string\n",
    "    )\n",
    "    logging.info('categorical column %s hash_bucket_size %d',\n",
    "                 key, hash_bucket_size)\n",
    "    return categorical_feature_column\n",
    "\n",
    "def create_categorical_feature_column_with_vocabulary_list(corpus_dict, vocabulary_size_dict, key):\n",
    "    corpus_size = vocabulary_size_dict[key]\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key,\n",
    "        list(corpus_dict[key].keys()),\n",
    "        dtype=tf.dtypes.string,\n",
    "        num_oov_buckets=max(1, corpus_size - len(corpus_dict[key]))\n",
    "    )\n",
    "    logging.info(\n",
    "        'categorical column with vocabular %s corpus_size %d', key, corpus_size)\n",
    "\n",
    "    return categorical_feature_column\n",
    "\n",
    "def create_embedding(vocabulary_size_dict, key, categorical_feature_column):\n",
    "    vocabulary_size = vocabulary_size_dict[key]\n",
    "    if vocabulary_size < 10:\n",
    "        logging.info(\n",
    "            'categorical column %s vocabulary_size %d - creating indicator column', key, vocabulary_size)\n",
    "        return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "    embedding_dimension = int(min(50, math.floor(6 * vocabulary_size**0.25)))\n",
    "    embedding_feature_column = tf.feature_column.embedding_column(\n",
    "        categorical_feature_column,\n",
    "        embedding_dimension)\n",
    "    return embedding_feature_column\n",
    "\n",
    "def create_linear_feature_columns():\n",
    "    return list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32) for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label')\n",
    "\n",
    "def create_categorical_embeddings_feature_columns(vocabulary_size_dict, embeddings_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    if embeddings_mode == EMBEDDINGS_MODE_TYPE.none:\n",
    "        return []\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.hashbucket:\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_hash_bucket(vocabulary_size_dict, key))\n",
    "            for key, _ in vocabulary_size_dict.items())\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "        logging.info('loading corpus dictionary')\n",
    "        corpus_dict = criteo_nbdev.data_reader.get_corpus_dict()\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_vocabulary_list(corpus_dict, vocabulary_size_dict, key))\n",
    "            for key, _ in corpus_dict.items())\n",
    "    else:\n",
    "        raise ValueError('invalid embedding_mode: {}'.format(embedding_mode))\n",
    "\n",
    "def create_feature_columns(embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    logging.info('loading vocabulary size dictionary')\n",
    "    vocabulary_size_dict = criteo_nbdev.data_reader.get_vocabulary_size_dict()\n",
    "    feature_columns = []\n",
    "    feature_columns.extend(create_linear_feature_columns())\n",
    "    feature_columns.extend(\n",
    "        create_categorical_embeddings_feature_columns(vocabulary_size_dict, embedding_mode))\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import criteo_nbdev.data_reader\n",
    "import nbdev.imports\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "def create_keras_model_sequential(embeddings_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    feature_columns = create_feature_columns(embeddings_mode)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns, name=\"feature_layer\")\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    Dropout = tf.keras.layers.Dropout\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            feature_layer,\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "\n",
    "    logging.info('compiling sequential keras model')\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_keras_model(\n",
    "    model, \n",
    "    model_dir, \n",
    "    epochs = 3,\n",
    "    dataset_source: DATASET_SOURCE_TYPE = DATASET_SOURCE_TYPE.bq,\n",
    "    dataset_size: DATASET_SIZE_TYPE = DATASET_SIZE_TYPE.tiny,\n",
    "    embeddings_mode: EMBEDDINGS_MODE_TYPE = EMBEDDINGS_MODE_TYPE.hashbucket,\n",
    "    distribution_strategy: DistributionStrategyType = None):\n",
    "    \n",
    "    log_dir = os.path.join(model_dir, \"logs\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1)\n",
    "\n",
    "    checkpoints_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    # crashing https://github.com/tensorflow/tensorflow/issues/27688\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "\n",
    "    callbacks = []\n",
    "    train_time_callback = TrainTimeCallback()\n",
    "\n",
    "    if DistributionStrategyType == DistributionStrategyType.TPU_STRATEGY:\n",
    "        # epoch and accuracy constants are not supported when training on TPU.\n",
    "        checkpoints_file_path = checkpoints_dir + \"/epochs_tpu.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        callbacks = [tensorboard_callback, checkpoint_callback, train_time_callback]\n",
    "    else:\n",
    "        if embeddings_mode == EMBEDDINGS_MODE_TYPE.manual or distribution_strategy == DistributionStrategyType.MULTI_WORKER_MIRRORED_STRATEGY:\n",
    "            # accuracy fails for adagrad\n",
    "            # for some reason accuracy is not available for EMBEDDINGS_MODE_TYPE.manual\n",
    "            # for some reason accuracy is not available for MultiWorkerMirroredStrategy\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}.hdf5\"\n",
    "        else:\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}-accuracy:{accuracy:.3f}.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        #callbacks = [tensorboard_callback, checkpoint_callback, train_time_callback]\n",
    "        callbacks = [tensorboard_callback, train_time_callback]\n",
    "\n",
    "    verbosity = 1 if nbdev.imports.in_ipython() else 2\n",
    "    if nbdev.imports.in_ipython():\n",
    "        callbacks.append(PlotLossesCallback())\n",
    "    \n",
    "    logging.info('training keras model')\n",
    "    training_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.training, embeddings_mode).repeat(epochs)\n",
    "    eval_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.validation, embeddings_mode).repeat(epochs)\n",
    "    \n",
    "    # steps_per_epoch and validation_steps are required for MultiWorkerMirroredStrategy\n",
    "    model.fit(\n",
    "        training_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=verbosity,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.training),\n",
    "        validation_data=eval_ds,\n",
    "        validation_steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation))\n",
    "\n",
    "    logging.info(\"done training keras model, evaluating model\")\n",
    "    loss, accuracy = model.evaluate(\n",
    "        eval_ds, \n",
    "        verbose=verbosity, \n",
    "        steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation), \n",
    "        callbacks=[tensorboard_callback])\n",
    "    logging.info(\"Eval - Loss: {}, Accuracy: {}\".format(loss, accuracy))\n",
    "    logging.info(model.summary())\n",
    "    logging.info(\"done evaluating keras model\")\n",
    "    return {'accuracy': accuracy, 'loss': loss}\n",
    "\n",
    "def train_and_evaluate_keras(\n",
    "    model_dir, \n",
    "    epochs = 3,\n",
    "    dataset_source: DATASET_SOURCE_TYPE = DATASET_SOURCE_TYPE.bq,\n",
    "    dataset_size: DATASET_SIZE_TYPE = DATASET_SIZE_TYPE.tiny,\n",
    "    embeddings_mode: EMBEDDINGS_MODE_TYPE = EMBEDDINGS_MODE_TYPE.hashbucket,\n",
    "    distribution_strategy: DistributionStrategyType = None):\n",
    "    \n",
    "    model = create_keras_model_sequential(embeddings_mode)\n",
    "    return train_and_evaluate_keras_model(\n",
    "        model,\n",
    "        model_dir,\n",
    "        epochs=epochs,\n",
    "        dataset_source=dataset_source,\n",
    "        dataset_size=dataset_size,\n",
    "        embeddings_mode=embeddings_mode,\n",
    "        distribution_strategy=distribution_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import criteo_nbdev.data_reader\n",
    "import nbdev.imports\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from criteo_nbdev.constants import *\n",
    "from gcp_runner.ai_platform_constants import *\n",
    "\n",
    "def keras_hp_search(\n",
    "    model_dir, \n",
    "    epochs = 3,\n",
    "    dataset_source: DATASET_SOURCE_TYPE = DATASET_SOURCE_TYPE.gcs,\n",
    "    dataset_size: DATASET_SIZE_TYPE = DATASET_SIZE_TYPE.tiny,\n",
    "    embeddings_mode: EMBEDDINGS_MODE_TYPE = EMBEDDINGS_MODE_TYPE.hashbucket,\n",
    "    distribution_strategy: DistributionStrategyType = None):\n",
    "\n",
    "    def build_model(hp):\n",
    "        feature_columns = create_feature_columns(embeddings_mode)\n",
    "        feature_layer = tf.keras.layers.DenseFeatures(feature_columns, name=\"feature_layer\")\n",
    "        Dense = tf.keras.layers.Dense\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(feature_layer)\n",
    "        model.add(Dense(hp.Choice('layer1', values=[50, 100, 200]), activation=tf.nn.relu, kernel_regularizer=kernel_regularizer)),\n",
    "        model.add(Dense(hp.Choice('layer2', values=[50, 100, 200]), activation=tf.nn.relu, kernel_regularizer=kernel_regularizer)),\n",
    "        model.add(Dense(1, activation=tf.nn.sigmoid, kernel_regularizer=kernel_regularizer))\n",
    "\n",
    "        logging.info('compiling sequential keras model')\n",
    "        # Compile Keras model\n",
    "        model.compile(\n",
    "          optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "          loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "          metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    training_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.training, embeddings_mode).repeat(epochs)\n",
    "    eval_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.validation, embeddings_mode).repeat(epochs)\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=30,\n",
    "        executions_per_trial=1,\n",
    "        directory=model_dir)\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "    tuner.search(training_ds,\n",
    "                 validation_data=eval_ds,\n",
    "                 epochs=3,\n",
    "                 verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def train_and_evaluate_estimator(\n",
    "    model_dir,\n",
    "    epochs = 3,\n",
    "    dataset_source: DATASET_SOURCE_TYPE = DATASET_SOURCE_TYPE.bq,\n",
    "    dataset_size: DATASET_SIZE_TYPE = DATASET_SIZE_TYPE.tiny,\n",
    "    embeddings_mode: EMBEDDINGS_MODE_TYPE = EMBEDDINGS_MODE_TYPE.hashbucket,\n",
    "    distribution_strategy: DistributionStrategyType = None):\n",
    "    \n",
    "    print(dataset_size)\n",
    "    logging.info('training for {} steps'.format(criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.training)))\n",
    "    config = tf.estimator.RunConfig(\n",
    "        train_distribute=distribution_strategy,\n",
    "        eval_distribute=distribution_strategy)\n",
    "    \n",
    "    feature_columns = create_feature_columns(embeddings_mode)\n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "        optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[598, 598, 598],\n",
    "        model_dir=model_dir,\n",
    "        config=config,\n",
    "        n_classes=2)\n",
    "    logging.info('training and evaluating estimator model')\n",
    "    training_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.training, embeddings_mode).repeat(epochs)\n",
    "    eval_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.validation, embeddings_mode).repeat(epochs) # why??\n",
    "\n",
    "    # Need to specify both max_steps and epochs. Each worker will go through epoch separately.\n",
    "    # see https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate?version=stable\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator,\n",
    "        train_spec=tf.estimator.TrainSpec(\n",
    "            input_fn=lambda: criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.training, embeddings_mode).repeat(epochs), \n",
    "            max_steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.training) * epochs),\n",
    "        eval_spec=tf.estimator.EvalSpec(\n",
    "            input_fn=lambda: criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.validation, embeddings_mode).repeat(epochs)))\n",
    "    logging.info('done evaluating estimator model')\n",
    "    \n",
    "    serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "        tf.feature_column.make_parse_example_spec(feature_columns))\n",
    "    estimator_base_path = os.path.join(model_dir, 'from_estimator')\n",
    "    estimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_estimator('./tmp', epochs=1, dataset_size=DATASET_SIZE_TYPE.tiny, embeddings_mode = EMBEDDINGS_MODE_TYPE.vocabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Imported. Visualize by running: tensorboard --logdir=./tmp/estimator_vocabular_logs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Imports a protobuf model as a graph in Tensorboard.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from tensorflow.python.client import session\n",
    "from tensorflow.python.framework import importer\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.platform import app\n",
    "from tensorflow.python.summary import summary\n",
    "from tensorflow.python.tools import saved_model_utils\n",
    "\n",
    "# Try importing TensorRT ops if available\n",
    "# TODO(aaroey): ideally we should import everything from contrib, but currently\n",
    "# tensorrt module would cause build errors when being imported in\n",
    "# tensorflow/contrib/__init__.py. Fix it.\n",
    "# pylint: disable=unused-import,g-import-not-at-top,wildcard-import\n",
    "try:\n",
    "    from tensorflow.contrib.tensorrt.ops.gen_trt_engine_op import *\n",
    "except ImportError:\n",
    "    pass\n",
    "# pylint: enable=unused-import,g-import-not-at-top,wildcard-import\n",
    "\n",
    "def import_to_tensorboard(model_dir, log_dir, tag_set):\n",
    "  \"\"\"View an imported protobuf model (`.pb` file) as a graph in Tensorboard.\n",
    "  Args:\n",
    "    model_dir: The location of the protobuf (`pb`) model to visualize\n",
    "    log_dir: The location for the Tensorboard log to begin visualization from.\n",
    "    tag_set: Group of tag(s) of the MetaGraphDef to load, in string format,\n",
    "        separated by ','. For tag-set contains multiple tags, all tags must be\n",
    "        passed in.\n",
    "  Usage:\n",
    "    Call this function with your model location and desired log directory.\n",
    "    Launch Tensorboard by pointing it to the log directory.\n",
    "    View your imported `.pb` model as a graph.\n",
    "  \"\"\"\n",
    "  with session.Session(graph=ops.Graph()) as sess:\n",
    "    input_graph_def = saved_model_utils.get_meta_graph_def(\n",
    "        model_dir, tag_set).graph_def\n",
    "    importer.import_graph_def(input_graph_def)\n",
    "\n",
    "    pb_visual_writer = summary.FileWriter(log_dir)\n",
    "    pb_visual_writer.add_graph(sess.graph)\n",
    "    print(\"Model Imported. Visualize by running: \"\n",
    "          \"tensorboard --logdir={}\".format(log_dir))\n",
    "    \n",
    "import_to_tensorboard('./tmp/from_estimator/1593547921', './tmp/estimator_vocabular_logs', 'serve')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
