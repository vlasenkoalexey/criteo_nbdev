{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "class TrainTimeCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = datetime.datetime.now()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logging.info('\\nepoch train time: (hh:mm:ss.ms) {}'.format(\n",
    "            datetime.datetime.now() - self.epoch_start_time))\n",
    "        if not self.params is None:\n",
    "            if 'steps' in self.params and self.params['steps']:\n",
    "                epoch_milliseconds = (datetime.datetime.now(\n",
    "                ) - self.epoch_start_time).total_seconds() * 1000\n",
    "                logging.info(\n",
    "                    '{} ms/step'.format(epoch_milliseconds / self.params['steps']))\n",
    "                if BATCH_SIZE is not None:\n",
    "                    logging.info('{} microseconds/example'.format(\n",
    "                        1000 * epoch_milliseconds / self.params['steps'] / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_categorical_feature_column_with_hash_bucket(corpus_dict, key):\n",
    "    corpus_size = len(corpus_dict[key])\n",
    "    hash_bucket_size = min(corpus_size, 100000)\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key,\n",
    "        hash_bucket_size,\n",
    "        dtype=tf.dtypes.string\n",
    "    )\n",
    "    logging.info('categorical column %s hash_bucket_size %d',\n",
    "                 key, hash_bucket_size)\n",
    "    return categorical_feature_column\n",
    "\n",
    "\n",
    "def create_categorical_feature_column_with_vocabulary_list(corpus_dict, key):\n",
    "    corpus_size = len(corpus_dict[key])\n",
    "    categorical_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key,\n",
    "        list(corpus_dict[key].keys()),\n",
    "        dtype=tf.dtypes.string,\n",
    "        num_oov_buckets=corpus_size\n",
    "    )\n",
    "    logging.info(\n",
    "        'categorical column with vocabular %s corpus_size %d', key, corpus_size)\n",
    "\n",
    "    return categorical_feature_column\n",
    "\n",
    "def create_embedding(vocabulary_size_dict, key, categorical_feature_column):\n",
    "    vocabulary_size = vocabulary_size_dict[key]\n",
    "    if vocabulary_size < 10:\n",
    "        logging.info(\n",
    "            'categorical column %s vocabulary_size %d - creating indicator column', key, vocabulary_size)\n",
    "        return tf.feature_column.indicator_column(categorical_feature_column)\n",
    "\n",
    "    embedding_dimension = int(min(50, math.floor(6 * vocabulary_size**0.25)))\n",
    "    embedding_feature_column = tf.feature_column.embedding_column(\n",
    "        categorical_feature_column,\n",
    "        embedding_dimension)\n",
    "    return embedding_feature_column\n",
    "\n",
    "def create_linear_feature_columns():\n",
    "    return list(tf.feature_column.numeric_column(field.name, dtype=tf.dtypes.float32) for field in CSV_SCHEMA if field.field_type == 'INTEGER' and field.name != 'label')\n",
    "\n",
    "def create_categorical_embeddings_feature_columns(corpus_dict, vocabulary_size_dict, embeddings_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    if embeddings_mode == EMBEDDINGS_MODE_TYPE.none:\n",
    "        return []\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.hashbucket:\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_hash_bucket(corpus_dict, key))\n",
    "            for key, _ in corpus_dict.items())\n",
    "    elif embeddings_mode == EMBEDDINGS_MODE_TYPE.vocabular:\n",
    "        return list(create_embedding(\n",
    "            vocabulary_size_dict,\n",
    "            key,\n",
    "            create_categorical_feature_column_with_vocabulary_list(corpus_dict, key))\n",
    "            for key, _ in corpus_dict.items())\n",
    "    else:\n",
    "        raise ValueError('invalid embedding_mode: {}'.format(embedding_mode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import criteo_nbdev.data_reader\n",
    "\n",
    "def create_feature_columns(embedding_mode: EMBEDDINGS_MODE_TYPE):\n",
    "    corpus_dict = criteo_nbdev.data_reader.get_corpus_dict()\n",
    "    vocabulary_size_dict = criteo_nbdev.data_reader.get_vocabulary_size_dict()\n",
    "    feature_columns = []\n",
    "    feature_columns.extend(create_linear_feature_columns())\n",
    "    feature_columns.extend(\n",
    "        create_categorical_embeddings_feature_columns(corpus_dict, vocabulary_size_dict, embedding_mode))\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from criteo_nbdev.constants import *\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_keras_model_sequential():\n",
    "    feature_columns = create_feature_columns(EMBEDDINGS_MODE_TYPE.hashbucket)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns, name=\"feature_layer\")\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    Dropout = tf.keras.layers.Dropout\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            feature_layer,\n",
    "            Dropout(0.3),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(598, activation=tf.nn.relu),\n",
    "            Dense(1, activation=tf.nn.sigmoid)\n",
    "        ])\n",
    "\n",
    "    logging.info('compiling sequential keras model')\n",
    "    # Compile Keras model\n",
    "    model.compile(\n",
    "        # cannot use Adagrad with mirroredstartegy https://github.com/tensorflow/tensorflow/issues/19551\n",
    "        # optimizer=tf.optimizers.Adagrad(learning_rate=0.05),\n",
    "        optimizer=tf.optimizers.SGD(learning_rate=0.05),\n",
    "        # optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n",
    "        # optimizer=tf.optimizers.Adam(),\n",
    "        #optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.1),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *\n",
    "from gcp_runner.ai_platform_constants import *\n",
    "import criteo_nbdev.data_reader\n",
    "import nbdev.imports\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "def train_and_evaluate_keras_model(\n",
    "    model, \n",
    "    model_dir, \n",
    "    epochs,\n",
    "    dataset_source: DATASET_SOURCE_TYPE,\n",
    "    dataset_size: DATASET_SIZE_TYPE,\n",
    "    embeddings_mode: EMBEDDINGS_MODE_TYPE,\n",
    "    distribution_strategy: DistributionStrategyType):\n",
    "    \n",
    "    log_dir = os.path.join(model_dir, \"logs\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        embeddings_freq=1)\n",
    "\n",
    "    checkpoints_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "    # crashing https://github.com/tensorflow/tensorflow/issues/27688\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "\n",
    "    callbacks = []\n",
    "    train_time_callback = TrainTimeCallback()\n",
    "\n",
    "    if DistributionStrategyType == DistributionStrategyType.TPU_STRATEGY:\n",
    "        # epoch and accuracy constants are not supported when training on TPU.\n",
    "        checkpoints_file_path = checkpoints_dir + \"/epochs_tpu.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        callbacks = [tensorboard_callback,\n",
    "                     checkpoint_callback, train_time_callback]\n",
    "    else:\n",
    "        if embeddings_mode == EMBEDDINGS_MODE_TYPE.manual or distribution_strategy == DistributionStrategyType.MULTI_WORKER_MIRRORED_STRATEGY:\n",
    "            # accuracy fails for adagrad\n",
    "            # for some reason accuracy is not available for EMBEDDINGS_MODE_TYPE.manual\n",
    "            # for some reason accuracy is not available for MultiWorkerMirroredStrategy\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}.hdf5\"\n",
    "        else:\n",
    "            checkpoints_file_path = checkpoints_dir + \\\n",
    "                \"/epochs:{epoch:03d}-accuracy:{accuracy:.3f}.hdf5\"\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoints_file_path, verbose=1, mode='max')\n",
    "        callbacks = [tensorboard_callback, checkpoint_callback, train_time_callback]\n",
    "\n",
    "    verbosity = 1 if nbdev.imports.in_ipython() else 2\n",
    "    logging.info('training keras model')\n",
    "    training_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.training, embeddings_mode).repeat(epochs)\n",
    "    eval_ds = criteo_nbdev.data_reader.get_dataset(dataset_source, dataset_size, DATASET_TYPE.validation, embeddings_mode).repeat(epochs)\n",
    "    \n",
    "    # steps_per_epoch and validation_steps are required for MultiWorkerMirroredStrategy\n",
    "    model.fit(\n",
    "        training_ds,\n",
    "        epochs=epochs,\n",
    "        verbose=verbosity,\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.training),\n",
    "        validation_data=eval_ds,\n",
    "        validation_steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation))\n",
    "\n",
    "    logging.info(\"done training keras model, evaluating model\")\n",
    "    loss, accuracy = model.evaluate(\n",
    "        eval_ds, \n",
    "        verbose=verbosity, \n",
    "        steps=criteo_nbdev.data_reader.get_steps_per_epoch(dataset_size, DATASET_TYPE.validation), \n",
    "        callbacks=[tensorboard_callback])\n",
    "    logging.info(\"Eval - Loss: {}, Accuracy: {}\".format(loss, accuracy))\n",
    "    logging.info(model.summary())\n",
    "    logging.info(\"done evaluating keras model\")\n",
    "    return {'accuracy': accuracy, 'loss': loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)\n",
    "\n",
    "# def train_keras_sequential(strategy, model_dir):\n",
    "#     return train_and_evaluate_keras_model(create_keras_model_sequential(), model_dir)\n",
    "\n",
    "# train_keras_sequential(None, './models/model1')\n",
    "\n",
    "def train_and_evaluate_keras_model_small(distribution_strategy=None, job_dir=None, **kwargs):\n",
    "    print('distribution_strategy:')\n",
    "    print(distribution_strategy)\n",
    "    print('job_dir:')\n",
    "    print(job_dir)\n",
    "    print('kwargs:')\n",
    "    print(kwargs)\n",
    "    train_and_evaluate_keras_model(create_keras_model_sequential(), job_dir, 2, DATASET_SOURCE_TYPE.bq, DATASET_SIZE_TYPE.tiny, EMBEDDINGS_MODE_TYPE.hashbucket, None)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "train_and_evaluate_keras_model_small(job_dir='models/model6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "import gcp_runner.local_runner\n",
    "gcp_runner.local_runner.run_python(train_and_evaluate_keras_model_small, python_binary='python3', job_dir='./models/model7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "import gcp_runner.local_runner\n",
    "gcp_runner.local_runner.run_docker(\n",
    "    train_and_evaluate_keras_model_small, \n",
    "    'gcr.io/alekseyv-scalableai-dev/criteo-nbdev', \n",
    "    build_docker_file='./Dockerfile', \n",
    "    job_dir='./models/model6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criteo_nbdev.core import skip; skip()\n",
    "# won't work until gcp_runner is published on pypi, also need to fix package dependencies\n",
    "\n",
    "import gcp_runner.ai_platform_runner\n",
    "gcp_runner.ai_platform_runner.run_package(\n",
    "     train_and_evaluate_keras_model_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir',\n",
    "     region='us-west1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To see job output in tensorboard, run following command:\n",
      "tensorboard --logdir=%s gs://alekseyv-scalableai-dev-criteo-model-bucket/models/model_mirrored_strategy_alekseyv_20200401_191035\n",
      "Building Docker image:\n",
      "docker build -f ./Dockerfile -t gcr.io/alekseyv-scalableai-dev/criteo-nbdev ./\n",
      "Sending build context to Docker daemon  2.976MB\n",
      "Step 1/14 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-1\n",
      " ---> 4c5f4efaf2dd\n",
      "Step 2/14 : WORKDIR /root\n",
      " ---> Using cache\n",
      " ---> 36acbff89f75\n",
      "Step 3/14 : ENV PROJECT_ID=alekseyv-scalableai-dev\n",
      " ---> Using cache\n",
      " ---> 4287a724b19d\n",
      "Step 4/14 : ENV GOOGLE_APPLICATION_CREDENTIALS=/root/service_account_key.json\n",
      " ---> Using cache\n",
      " ---> c5492faca793\n",
      "Step 5/14 : COPY service_account_key.json /root/\n",
      " ---> Using cache\n",
      " ---> 45bb06aae86f\n",
      "Step 6/14 : RUN pip install nbdev\n",
      " ---> Using cache\n",
      " ---> b2097138828d\n",
      "Step 7/14 : ENV KMP_AFFINITY=\"\"\n",
      " ---> Using cache\n",
      " ---> 3c6c699cb173\n",
      "Step 8/14 : ENV TF_DISABLE_MKL=1\n",
      " ---> Using cache\n",
      " ---> bcc1fb62d4ef\n",
      "Step 9/14 : ADD \"https://www.random.org/cgi-bin/randbyte?nbytes=10&format=h\" skipcache\n",
      "\n",
      "\n",
      " ---> 87e4e2abab62\n",
      "Step 10/14 : RUN git clone https://github.com/vlasenkoalexey/gcp_runner\n",
      " ---> Running in 7252fbbd9dd1\n",
      "\u001b[91mCloning into 'gcp_runner'...\n",
      "\u001b[0mRemoving intermediate container 7252fbbd9dd1\n",
      " ---> fff27bdfd166\n",
      "Step 11/14 : RUN pip install -e gcp_runner\n",
      " ---> Running in e1ca751bb710\n",
      "Obtaining file:///root/gcp_runner\n"
     ]
    }
   ],
   "source": [
    "#from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "import gcp_runner.ai_platform_runner\n",
    "gcp_runner.ai_platform_runner.run_docker_image(\n",
    "     train_and_evaluate_keras_model_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/models/model_mirrored_strategy_{username}_{datetime}',\n",
    "     master_image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     region='us-west1',\n",
    "     distribution_strategy_type = gcp_runner.ai_platform_constants.DistributionStrategyType.MIRRORED_STRATEGY,\n",
    "     use_distribution_strategy_scope=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from criteo_nbdev.core import skip; skip()\n",
    "import gcp_runner.ai_platform_constants\n",
    "\n",
    "import gcp_runner.kubernetes_runner\n",
    "gcp_runner.kubernetes_runner.run_docker_image(\n",
    "     train_and_evaluate_keras_model_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/model_mirrored_strategy_{username}_{datetime}',\n",
    "     image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     distribution_strategy_type = gcp_runner.ai_platform_constants.DistributionStrategyType.MIRRORED_STRATEGY,\n",
    "     use_distribution_strategy_scope=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
