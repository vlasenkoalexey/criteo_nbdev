{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running distributed training on Google Cloud Platform using [gcp_runner](https://github.com/vlasenkoalexey/gcp_runner) and [nbdev](https://github.com/fastai/nbdev/tree/master/nbdev)\n",
    "\n",
    "> This projects demonstrates how to run distributed training for TensorFlow Keas and Estimator columnar models on Google Cloud Platform. As a data source public [Criteo Kaggle dataset](https://www.kaggle.com/c/criteo-display-ad-challenge/data) is used.\n",
    "Project also demonstrates how to import and process data using BigQuery and train models using [BigQuery reader](https://github.com/tensorflow/io/tree/master/tensorflow_io/bigquery). Jupyter notebooks in this projects are converted into Python package using [nbdev](https://github.com/fastai/nbdev/tree/master/nbdev), and [gcp_runner](https://github.com/vlasenkoalexey/gcp_runner) provides a straightforward way of running code from Jupyter notebooks on Google Cloud AI Platform or on Kubernetes without any modifications.\n",
    "Models can be trained in all distributed modes supported by TensorFlow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In order to run this samples locally, just clone this repository and run Jupyter lab or Jupyter notebook from the same folder.\n",
    "\n",
    "Jupyter lab can be installed as following:\n",
    "```sh\n",
    "pip install jupyterlab\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "!!! IMPORTANT !!!\n",
    "\n",
    "In order to run samples on Google Cloud, it is necessary to install and configure Google Cloud SDK.\n",
    "Follow steps from `setup.ipynb` to do that.\n",
    "\n",
    "\n",
    "If you change some of the code on any other Jupyter notebook, use following command to regenerate package and reload all Python modules: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp_runner.core import export_and_reload_all\n",
    "export_and_reload_all(silent=True, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training locally\n",
    "\n",
    "Import modules, initialize dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from criteo_nbdev import trainer\n",
    "from criteo_nbdev.constants import *\n",
    "from gcp_runner.ai_platform_constants import *\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a helper function in a cell marked with `#export` attribute, notebook also has to have `#default_exp <name>` defined at the top. See `05_trainer.ipynb` for trainer code definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_and_evaluate_keras_small(distribution_strategy=None, job_dir=None, **kwargs):\n",
    "    print(tensorflow_io.version.VERSION)\n",
    "    trainer.train_and_evaluate_keras(job_dir, epochs=3, dataset_size=DATASET_SIZE_TYPE.tiny, distribution_strategy=distribution_strategy)\n",
    "    \n",
    "def train_and_evaluate_estimator_small(distribution_strategy=None, job_dir=None, **kwargs):\n",
    "    trainer.train_and_evaluate_estimator(job_dir, epochs=3, dataset_size=DATASET_SIZE_TYPE.tiny, distribution_strategy=distribution_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training in Jupyter notebook as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp_runner.core import export_and_reload_all\n",
    "export_and_reload_all(silent=True, ignore_errors=False)\n",
    "train_and_evaluate_keras_small(job_dir='./models/model_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running same code locally, but in a Docker container. Only special requirement to be able to run this, is to have gcp_runner project installed in Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp_runner.core import export_and_reload_all\n",
    "export_and_reload_all(silent=True, ignore_errors=False)\n",
    "\n",
    "import gcp_runner.local_runner\n",
    "gcp_runner.local_runner.run_docker(\n",
    "    train_and_evaluate_keras_small, \n",
    "    'gcr.io/alekseyv-scalableai-dev/criteo-nbdev', \n",
    "    build_docker_file='./Dockerfile', \n",
    "    job_dir='./models/model_docker_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this function is doing without running any code, add `dry_run=True`.\n",
    "In this case it is running following:\n",
    "\n",
    "```sh\n",
    "docker build -f ./Dockerfile -t gcr.io/alekseyv-scalableai-dev/criteo-nbdev ./\n",
    "docker run -v /Users/alekseyv/vlasenkoalexey/criteo_nbdev/criteo_nbdev:/criteo_nbdev \\ \n",
    "    gcr.io/alekseyv-scalableai-dev/criteo-nbdev \\\n",
    "    python -u -m gcp_runner.entry_point \\\n",
    "    --module-name=criteo_nbdev.index \\\n",
    "    --function-name=train_and_evaluate_keras_small \\\n",
    "    --job-dir=./models/model_docker_1\n",
    "```\n",
    "\n",
    "In other words, it is building a Docker container which has a python module `criteo_nbdev.index` with a function to execute\n",
    "`train_and_evaluate_keras_small`. And it passes this information along with additional arguments to the gcp_runner.entry_point that loads\n",
    "package and invokes specified function with arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on Google Cloud AI Platform\n",
    "\n",
    "If everything running in Docker locally worked as expected, and Google Cloud SDK is installed and configured, same code can be runned on Cloud AI Platform as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.ai_platform_runner\n",
    "\n",
    "gcp_runner.ai_platform_runner.run_docker_image(\n",
    "     train_and_evaluate_keras_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/models/model_{username}_{datetime}',\n",
    "     master_image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     region='us-west1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And same code can be executed using any distributed strategy whithout modifications.\n",
    "For Keras, you can use `use_distribution_strategy_scope=True` argument. It argument is set to true, code is going to be executed within that strategy scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.ai_platform_runner\n",
    "\n",
    "gcp_runner.ai_platform_runner.run_docker_image(\n",
    "     train_and_evaluate_keras_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/models/model_ms_{username}_{datetime}',\n",
    "     master_image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     region='us-west1',\n",
    "     distribution_strategy_type = DistributionStrategyType.MIRRORED_STRATEGY,\n",
    "     use_distribution_strategy_scope=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case following gcloud command is executed:\n",
    "\n",
    "```sh\n",
    "gcloud ai-platform jobs submit training ai_platform_runner_train_docker_20200402_182501 \\ \n",
    " --stream-logs \\ \n",
    " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/models/model_ms_alekseyv_20200402_182501 \\ \n",
    " --region=us-west1 \\ \n",
    " --scale-tier=custom \\ \n",
    " --master-machine-type=n1-standard-4 \\ \n",
    " --master-image-uri=gcr.io/alekseyv-scalableai-dev/criteo-nbdev \\ \n",
    " --master-accelerator=count=2,type=nvidia-tesla-k80 \\ \n",
    " --use-chief-in-tf-config=True \\ \n",
    " -- python -u -m gcp_runner.entry_point \\ \n",
    " --module-name=criteo_nbdev.index \\ \n",
    " --function-name=train_and_evaluate_keras_small \\ \n",
    " --job-dir=gs://alekseyv-scalableai-dev-criteo-model-bucket/models/model_ms_alekseyv_20200402_182501 \\ \n",
    " --distribution-strategy-type=tf.distribute.MirroredStrategy \\ \n",
    " --use-distribution-strategy-scope\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on Kubernetes\n",
    "\n",
    "Usually it takes few minutes for Job to be provisioned on Google Cloud AI platform, which is not perfect for experimentation and iterative development. \n",
    "\n",
    "In order to iterate faster, it is possible to setup a Kubernetes cluster of desired configuration, and use gcp_runner to run code on Kubernetes in a same manner. Distributed training is also supported on Kubernetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.ai_platform_constants\n",
    "\n",
    "import gcp_runner.kubernetes_runner\n",
    "gcp_runner.kubernetes_runner.run_docker_image(\n",
    "     train_and_evaluate_keras_model_small,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/model_mirrored_strategy_{username}_{datetime}',\n",
    "     image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     distribution_strategy_type = gcp_runner.ai_platform_constants.DistributionStrategyType.MIRRORED_STRATEGY,\n",
    "     use_distribution_strategy_scope=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed hyper parameter tuning using Keras Tuner\n",
    "\n",
    "[Keras Tuner](https://keras-team.github.io/keras-tuner/) provides very user friendly API for hyper parameter search, and also supports [distributed tuning](https://keras-team.github.io/keras-tuner/tutorials/distributed-tuning/).\n",
    "All that is necessary in order to run tuning on multiple machines in parallel is to set few environment arguments.\n",
    "gcp_runner takes care of that which makes running distributed parameter search very straighforward.\n",
    "\n",
    "See 05_trainer.ipynb for hyper model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev import trainer\n",
    "\n",
    "def run_keras_hp_search(job_dir=None, **kwargs):\n",
    "    trainer.keras_hp_search(job_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on AI platform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcp_runner.ai_platform_constants import *\n",
    "import gcp_runner.ai_platform_runner\n",
    "\n",
    "gcp_runner.ai_platform_runner.run_docker_image(\n",
    "     run_keras_hp_search,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/model_mirrored_strategy_{username}_{datetime}',\n",
    "     master_image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     master_machine_type=MachineType.N1_STANDARD_16,\n",
    "     worker_machine_type=MachineType.N1_STANDARD_16,\n",
    "     worker_machine_count=6,\n",
    "     region='us-west1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on Kubernetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcp_runner.kubernetes_runner\n",
    "gcp_runner.kubernetes_runner.run_docker_image(\n",
    "     run_keras_hp_search,\n",
    "     'gs://alekseyv-scalableai-dev-criteo-model-bucket/test-job-dir/model_mirrored_strategy_{username}_{datetime}',\n",
    "     image_uri='gcr.io/alekseyv-scalableai-dev/criteo-nbdev',\n",
    "     build_docker_file='./Dockerfile',\n",
    "     worker_machine_count=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
