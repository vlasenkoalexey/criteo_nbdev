{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from criteo_nbdev.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import GoogleAPIError\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def create_bigquery_dataset_if_necessary(dataset_id):\n",
    "    # Construct a full Dataset object to send to the API.\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    dataset = bigquery.Dataset(\n",
    "        bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id))\n",
    "    dataset.location = LOCATION\n",
    "\n",
    "    try:\n",
    "        dataset = client.create_dataset(dataset)  # API request\n",
    "        return True\n",
    "    except GoogleAPIError as err:\n",
    "        if err.code != 409:  # http_client.CONFLICT\n",
    "            raise\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_data_into_bigquery(source_uris, dataset_id, table_id):\n",
    "    create_bigquery_dataset_if_necessary(dataset_id)\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.field_delimiter = '\\t'\n",
    "    job_config.schema = CSV_SCHEMA\n",
    "\n",
    "    load_job = client.load_table_from_uri(\n",
    "        source_uris, table_ref, job_config=job_config\n",
    "    )\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "\n",
    "    try:\n",
    "        load_job.result()  # Waits for table load to complete.\n",
    "        print(\"Job finished.\")\n",
    "    except Exception as err:\n",
    "        print(load_job.errors)\n",
    "        print(err)\n",
    "\n",
    "    destination_table = client.get_table(table_ref)\n",
    "    print(\"Loaded {} rows.\".format(destination_table.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from criteo_nbdev.constants import *\n",
    "from criteo_nbdev.core import skip\n",
    "\n",
    "skip()\n",
    "\n",
    "print(URL)\n",
    "print(DATASET_ID)\n",
    "print(TABLE_ID)\n",
    "\n",
    "load_data_into_bigquery(URL, DATASET_ID, TABLE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%%bigquery --project alekseyv-scalableai-dev --verbose\n",
    "SELECT\n",
    "count(*)\n",
    "FROM `alekseyv-scalableai-dev.criteo_kaggle_2.days`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import re\n",
    "from tensorflow.python.platform import gfile\n",
    "from criteo_nbdev.constants import *\n",
    "\n",
    "def _get_file_names(file_pattern):\n",
    "    if isinstance(file_pattern, list):\n",
    "        if not file_pattern:\n",
    "            raise ValueError(\"File pattern is empty.\")\n",
    "        file_names = []\n",
    "        for entry in file_pattern:\n",
    "            file_names.extend(gfile.Glob(entry))\n",
    "    else:\n",
    "        file_names = list(gfile.Glob(file_pattern))\n",
    "    return file_names\n",
    "        \n",
    "def _get_file_suffix(file_name):\n",
    "    return int(re.search(GCS_FOLDER + 'train_(\\d+)', file_name).group(1))\n",
    "\n",
    "def get_file_names_with_validation_split(dataset_size: DATASET_SIZE_TYPE, dataset_type:DATASET_TYPE, validation_split):\n",
    "    file_names = _get_file_names(GCS_FOLDER + 'train_0*')\n",
    "    modulo = int(len(file_names) * validation_split)\n",
    "    large_dataset_file_names = list(\n",
    "        file_name for file_name in file_names \\\n",
    "        if (dataset_type == DATASET_TYPE.training) ^ (_get_file_suffix(file_name) % modulo == 0))\n",
    "    if dataset_size == DATASET_SIZE_TYPE.full:\n",
    "        return large_dataset_file_names\n",
    "    \n",
    "    modulo = None\n",
    "    if dataset_size == DATASET_SIZE_TYPE.small:\n",
    "        modulo = 10\n",
    "    elif dataset_size == DATASET_SIZE_TYPE.tiny:\n",
    "        modulo = 100\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset_size:%s', dataset_size)\n",
    "    small_dataset_files = list(\n",
    "        file_name for (idx, file_name) in enumerate(large_dataset_file_names) if idx % modulo == 0)\n",
    "    return small_dataset_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import gcp_runner.core\n",
    "gcp_runner.core.export_and_reload_all(silent=True)\n",
    "\n",
    "get_file_names_with_validation_split(DATASET_SIZE_TYPE.tiny, DATASET_TYPE.validation, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from criteo_nbdev.core import skip; skip()\n",
    "\n",
    "file_names = get_file_names_with_validation_split(DATASET_SIZE_TYPE.small, DATASET_TYPE.training, 0.2)\n",
    "print(file_names)\n",
    "load_data_into_bigquery(file_names, DATASET_ID, 'training_small')\n",
    "\n",
    "file_names = get_file_names_with_validation_split(DATASET_SIZE_TYPE.small, DATASET_TYPE.validation, 0.2)\n",
    "print(file_names)\n",
    "load_data_into_bigquery(file_names, DATASET_ID, 'validation_small')\n",
    "\n",
    "file_names = get_file_names_with_validation_split(DATASET_SIZE_TYPE.full, DATASET_TYPE.validation, 0.2)\n",
    "print(file_names)\n",
    "load_data_into_bigquery(file_names, DATASET_ID, 'validation_full')\n",
    "\n",
    "file_names = get_file_names_with_validation_split(DATASET_SIZE_TYPE.full, DATASET_TYPE.training, 0.2)\n",
    "print(file_names)\n",
    "load_data_into_bigquery(file_names, DATASET_ID, 'training_full')\n",
    "\n",
    "file_names = get_file_names_with_validation_split(DATASET_SIZE_TYPE.tiny, DATASET_TYPE.validation, 0.2)\n",
    "print(file_names)\n",
    "load_data_into_bigquery(file_names, DATASET_ID, 'validation_tiny')\n",
    "\n",
    "file_names = get_file_names_with_validation_split(DATASET_SIZE_TYPE.tiny, DATASET_TYPE.training, 0.2)\n",
    "print(file_names)\n",
    "load_data_into_bigquery(file_names, DATASET_ID, 'training_tiny')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
